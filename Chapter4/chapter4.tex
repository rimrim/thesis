\chapter{The Second Protocol - Covering Circuit Privacy}
\label{chap:renyiDivergence}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

\section{Introduction}
\label{sec:secProcIntro}

\section{Previous Works}
\label{sec:secProcPrevious}

\section{Renyi Divergence Analysis technique}
\label{sec:secProcRenyi}
% \subsection{Renyi Divergence and its application in noise masking}
% \label{sec:Renyi_original}
Consider the a product ciphertexts result in BV cryptosystem (section \ref{sec:BVScheme}).
\[
mult(c,c') = (\mathbf{c_0}\mathbf{c_0'}, \mathbf{c_0}\mathbf{c_1'} + \mathbf{c_1}\mathbf{c_0'}, \mathbf{c_1}\mathbf{c_1'})
\]
The noise term of this result ciphertext correlates to both $\mathbf{s}$ and $\mathbf{m}$.
In many contexts, this leakage might be fine for a genuine user with a secret key because s/he is supposed to know the key and decrypt the message.
However, in many other contexts, especially in multi-factor authentication
scenarios, this is not the case. For instance, in our system, we do not want the
client to know the Hamming Distance result, so that an attacker with a stolen
device and a secret key cannot derive information about the data stored in the
server. We propose to mask this leakage by homomorphically adding the ciphertext
$Enc(HD)$ with $Enc(r)$ to refresh the noise terms, together with masking the Hamming Distance. The question is how much can
we shift the original noise distribution to preserve correctness while providing
the new security measure.

Let $D_1$ and $D_2$ be the probability distributions of the original noise in the ciphertext $Enc(HD)$ and the new shifted noise of $Enc(HD+r)$. We observe that the
2 distributions are identical Gaussian distribution with the same standard deviation
$\sigma$ and different means. Let $r_0$ be the shifted in means of $D_1$ and $D_2$.
Our goal is to set up parameter $r_0$ such that $D_1$ and $D_2$ are computational
indistinguishable while keeping other parameters of the cryptosystem within practical
performance
thresholds. Statistical Distance (SD) is normally used to measure the difference of distributions and is generally bounded by
\[
SD(D_1, D_2) = \frac{1}{2}\sum_{x\in X}|D_1(x) - D_2(x)| \leq K\times \frac{r_0}
{\sigma}
\]
Where $K$ is a constant. We quickly see that in order for $SD$ to be indistinguishable ($SD < \epsilon \approx \frac{1}{2^\lambda}$, where $\lambda$ is
the security parameter), the standard deviation of the initial noise needs to be really large, which is $\sigma \geq Kr_02^\lambda$.

In the work of \cite{bai2015improved}, the authors proposed Renyi Divergence (RD) as
an alternative to measure distributions closeness and its applications to security proofs. $R_a(D_1\|D_2)$ of order $a$
between $D_1$ and $D_2$ is defined as the expected value of $(D_1(x)/D_2(x))^{a-1}$ over the randomness of $x$ sampled from $D_1$.
\[
R_a(D_1\|D_2) = \left( \sum_{x \in D_1}\frac{D_1(x)^a}{D_2(x)^{a-1}} \right)^
{\frac{1}{a-1}}
\]
Similar to SD, RD is useful in our context with its \textit{Probability Preservation} property (we refer readers to \cite{bai2015improved} for detailed formal
descriptions): Given $D_1$ and $D_2$ as described, for any event $E$, for instance, we want to look at $P(E)$ is the winning probability of the attacker in the distinguishing game, the probability of the event with respect to $D_2$ is bounded by
\begin{align}
\label{eq:renyi}
D_2(E) \geq D_1(E)^{\frac{a}{a-1}}/RD_a(D1\|D_2)
\end{align}
Particularly, if we look at the second order ($a = 2$) of RD like previous works, we would have
$
D_2(E) \geq D_1(E)^2/RD_2(D1\|D_2)
$.
Provided that the distribution functions of $D_1$ and $D_2$ are discrete Gaussian on lattices, which is of the form $\rho_\sigma(x) = \frac{1}{\sigma}e^{-\pi\frac{x^2}
	{\sigma^2}}$ , we have $RD_2(D_1\|D_2) = e^{2\pi \frac{r_0^2}{\sigma^2}}
\approx e^{2\pi}$, when $r_0$ is much smaller than $\sigma$. That means when switching from $D_1$ to $D_2$, the success probability of $E$ will be at least the old probability to the power of 2 divided by some constant. This squaring factor
brings a big trade-off, for example, in our protocol, we would need to use $FAR = 2^{-20}$ in the non-privacy biometric settings to get $FAR=2^{-10}$ in our scheme.

We aim at a solution to remove this factor. The idea is to look at $RD_\infty$ in stead of $RD_2$: from equation (\ref{eq:renyi}), we can see that when $a$ is large, $\frac{a}{a-1}$ becomes 1. However, for usual Gaussian distributions, $RD_\infty$ is also infinity (not a constant $e^{2\pi}$ like in $RD_2$ when $a=2$). This is due to the ratios $\frac{D_1(x)}{D_2(x)}$ become large when samplings are done in the extreme tails of the distributions. Our idea is to truncate the distribution when doing noise sampling: if we get a noise value that is too far in the tail, we reject and sample again. As a result, the truncated distribution grows slightly, that means, the small noises have a bit higher probability when sampling, which does not have a big impact in security.

For the following analysis, let $D_1$ to be a discrete Gaussian on $\mZ$ with deviation parameter $\sigma$ shifted by the constant $r_0 \in \mZ$, while $D_2$ is a discrete Gaussian on $\mZ$ with dev. par. $\sigma$ centered on zero, i.e. $D_1 = D_{\mZ,\sigma} + r_0$ and $D_2 = D_{\mZ,\sigma}$, where $D_{\mZ,\sigma}(x) = e^{-\pi \cdot x^2/\sigma^2}/\sum_{z \in \mZ} e^{-\pi \cdot z^2/\sigma^2}$ for $x \in \mZ$. To allow us to use $RD_{\infty}$ we use tail-cut variants $D_1^{(cut)}$ and $D_2^{(cut)}$ of $D_1$ and $D_2$, respectively with parameter $k$. The parameter $k$ defines where $D_1$ and $D_2$ are cut at, for example, we can set $k=3$ to cut the distributions at 3 deviation parameters from the mean. So, we let $D_{\mZ,\sigma}^{(cut)}$ denote distribution $D_{\mZ,\sigma}$ tail-cutted to the interval $[-k \cdot \sigma, k \cdot \sigma]$ by rejection sampling. We let $D_1^{(cut)} = D_{\mZ,\sigma}^{(cut)} + r_0$ and $D_2^{(cut)} = D_{\mZ,\sigma}^{(cut)}$. Notice that the supports of $D_1^{(cut)}$ and $D_2^{(cut)}$ are different, namely $Supp(D_1^{(cut)}) = [-k\sigma+r_0,k\sigma+r_0]$ while $Supp(D_2^{(cut)}) = [-k\sigma,k\sigma]$. We assume, without loss of generality, that $r_0 > 0$. We would like to switch from distribution $D_1^{(cut)}$ to $D_2^{(cut)}$, but unfortunately $R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)})$ is not finite since $Supp(D_1^{(cut)})$ is not a subset of $Supp(D_2^{(cut)})$. To satisfy the latter condition, we first switch from $D_1^{(cut)}$ to $\overline{D}_1^{(cut)}$ by further cutting (by rejection sampling) the positive tail of $D_1^{(cut)}$ to ensure it does not go beyond the $k \sigma$ upper bound on tail of $D_2^{(cut)}$, and use a (mild condition) statistical distance step to lower bound $\overline{D}_1^{(cut)}(E)$. Then, in a second step using $Supp(\overline{D}_1^{(cut)})=[-k\sigma+r_0, k\sigma] \subseteq [-k\sigma,k\sigma] = Supp(D_2^{(cut)})$, we derive a finite upper bound on $R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)})$ to lower bound $D_2^{(cut)}(E)$. Details follow.
\begin{sloppypar}
\textbf{First SD Step.} Since $Supp(D_1^{(cut)})$ is transformed into $\overline{D}_1^{(cut)}$ by rejection and resampling if a sample of $Supp(D_1^{(cut)})$ falls in $(k\sigma,k\sigma+r_0]$, we have $SD(D_1^{(cut)}, \overline{D}_1^{(cut)}) \leq D_1^{(cut)}((k\sigma,k\sigma+r_0]) = D_2^{(cut)}((k\sigma-r_0,k\sigma]) = D_{\mZ,\sigma}((k\sigma-r_0,k\sigma])/C_2$, where $C_2 = D_{\mZ,\sigma}([-k\sigma,k\sigma])$. Now, we have
$$
\Delta \defeq \frac{D_{\mZ,\sigma}((k\sigma-r_0,k\sigma])}{C_2} = \frac{\sum_{z \in (k\sigma-r_0,k\sigma])} \e^{-\pi z^2/\sigma^2}}{\sum_{z \in [-k\sigma,k\sigma])} \e^{-\pi z^2/\sigma^2}}.
$$
For the numerator, we have an upper bound $\sum_{z \in (k\sigma-r_0,k\sigma])} \e^{-\pi z^2/\sigma^2} \leq \int^{\infty}_{k\sigma-r_0} \e^{-\pi z^2/\sigma^2} dz \leq \sigma \cdot \e^{-\pi (k\sigma-r_0)^2/\sigma^2}$, using the standard normal distribution upper bound $\int^{\infty}_{\gamma} \frac{1}{\sqrt{2\pi} \sigma} \cdot \e^{- z^2/\sigma^2} dz \leq \e^{-\gamma^2/(2\sigma^2)}$ for $\gamma \geq 0$. For the denominator, we have a lower bound $\sum_{z \in [-k\sigma,k\sigma]} \e^{-\pi z^2/\sigma^2} \geq 2 \cdot \sum_{z \in [0,k\sigma])} \e^{-\pi z^2/\sigma^2} \geq 2 \cdot (\int^{\infty}_{0} \e^{-\pi z^2/\sigma^2} dz - \int^{\infty}_{k\sigma} \e^{-\pi z^2/\sigma^2} dz) \geq \sigma \cdot (1 - 2 \cdot \e^{-\pi (k\sigma-r_0)^2/\sigma^2})$. Therefore, $SD(D_1^{(cut)}, \overline{D}_1^{(cut)}) \leq \Delta \leq \delta' / (1-2\delta') \leq 2\delta'$ if $\delta' \leq 1/4$, where $\delta' = \e^{-\pi (k\sigma-r_0)^2/\sigma^2}$. Defining $\delta = 2\delta'$, we have $\Delta \leq \delta$ if $\delta \leq 1/8$ and the conditions $r_0 \leq \sigma$ and $k \geq 1 + \sqrt{1/\pi \cdot \ln(2/\delta)}$ hold. Therefore, for any event $E$ we have $\overline{D}_1^{(cut)}(E) \geq D_1^{(cut)}(E) - \delta$.
\end{sloppypar}
%with probability $D_1^{(cut)}(E)$ \geq \varepsilon$, we have can take $\delta = \var{\epsilon This tells us how to set the tail cut parameter $k$.

\textbf{Second RD step.} The desired RD of order $\infty$ is defined by
\[
R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)})) = \max_{x \in [-k\sigma+r_0,k\sigma]} \frac{\overline{D}_1^{(cut)}}
{D_2^{(cut)}(x)}.
\]
Observe that, for each $x \in [-k\sigma+r_0,k\sigma]$, we have $\overline{D}_1^{(cut)}(x) = C \cdot D_1^{(cut)}(x)$, where the normalization constant $C = \frac{1}{1-D_1^{(cut)}((k\sigma,k\sigma+r_0])} = \frac{1}{1-D_2^{(cut)}((-k\sigma+r_0,k\sigma])} = \frac{1}{1-\Delta}$, where $\Delta$ is defined and upper bounded by $\delta$ above under the assumed conditions on $k$ and $\delta$. Since the $D_1^{(cut)}(x)$ and $D_2^{(cut)}(x)$ are shifts of each other, they have the same rejection sampling normalization constant with respect to $D_1$ (resp. $D_2$). Therefore, $D_1^{(cut)}(x)/D_2^{(cut)}(x)=D_1(x)/D_2(x)$ for each $x$ in the support of both $D_1^{(cut)}$ and $D_2^{(cut)}$, and we have
%$D_1(x)$ and $D_2(x)$ divided by some constant factor $C_1$ and $C_2$ (to keep the cumulative probability area to be 1) . In our context, as $D_1(x)$ and $D_2(x)$ are the same distributions with different means, so $C_1 = C_2$.
\begin{align*}
R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)}) &\leq \frac{1}{1-\delta} \cdot \max_{x \in [-k\sigma+r_0,k\sigma]} \frac{D_1(x)}
{D_2(x)}\\
&= \max_{x \in [-k\sigma,k\sigma]}\frac{e^{\frac{-\pi(x-r_0)^2}{\sigma^2}}}{e^{-\pi\frac{x^2}{\sigma^2}}}
\\
&= e^{\pi \cdot r_0^2/\sigma^2} \cdot \max_{x \in [-k\sigma+r_0,k\sigma]}e^{\frac{2\pi r_0}
	{\sigma^2}x}
\end{align*}
This is an exponential function and we get the max value at $x = k\sigma$:
$$
R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)}) = e^{1/(1-\delta)} \cdot e^{\pi \cdot r_0^2/\sigma^2 + 2\pi k \cdot r_0/\sigma}.
$$
Since $0<\delta \leq 1/8$, the first factor above is $\leq 1+2\delta \leq e^{2\delta}$. Also, a simple computation shows that the second factor is $\leq e$ if the condition $\sigma/r_0 \geq 4 \pi \cdot k$ is satisfied using $k \geq 1$. We conclude, under the assumed parameter conditions that $R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)})) \leq e^{1+2\delta} = c'(\delta)$ is constant for constant $\delta>0$, so that, by the RD probability preservation property $D_2^{(cut)}(E) \geq \frac{1}{c(\delta)} \cdot \overline{D}_1^{(cut)}(E) \geq \frac{1}{c(\delta)} \cdot (D_1^{(cut)}(E) - \delta)$. Note that if $D_1^{(cut)}(E)=\varepsilon$, then by choosing $\delta = \varepsilon/2$, we get $D_2^{(cut)}(E) \geq \frac{1}{2c(\delta)} \cdot \varepsilon$, and we only need $k \geq 1 + \sqrt{1/\pi \cdot \ln(2/\delta)}$ and $\sigma/r_0$ logarithmic in $1/\delta$,  much smaller than $\sigma/r_0$ linear in $1/\delta$, which we would need if we were to use the `SD only' analysis approach.

The above discussion immediately generalizes from the one-dimensional case of discrete Gaussian samples over $\mZ$ to the $m$-dimensional case of discrete Gaussian samples over $\mZ^m$ due to the independence of the $m$ coordinates. The only changes to the above argument is that the statistical distance in the `SD step' can multiply by at most a factor $m$, whereas the RD in the `RD step' above gets raised to the $m$'th power, where we replace $r_0$ by $\|\vec{r}_0\|_{\infty}$. We compensate for this by replacing the bound $\delta$ on $\Delta$ in the above analysis by the bound $\delta/m$. We have therefore proved the following result used in our impersonation security proof, which improved upon the $R_2$-based analogue result for shifted Gaussians stated in~\cite{langlois2014gghlite}.

%\keq Note that when $r_0$ is much smaller than $\sigma$, the first term $e^{\frac{\pi r_0^2}{\sigma^2}} \approx 1$. However, if $r_0$ is too small compared to $\sigma$, then the
%second term is at most $e^{2k\pi}$, which is also very large in the relation of equation (\ref{eq:renyi}). Or, we can also choose parameters such as $k = 3$ and $\frac{r_0}{\sigma} = \frac{1}{6}$, then the trade-off factor becomes much smaller, $e^{\pi}$. In conclusion, by applying infinity order RD to measure the truncated distributions closeness, we might obtain better parameters ($\sigma \geq 6r_0$, for example, compared to $\sigma \geq Kr_02^\lambda$ in SD approach) while providing \textit{Probability Preservation}, or
%security against distinguishing adversaries.

\begin{lemma} \label{le:Renyi}
For integer $m \geq 1$, real $\sigma>0$,$k \geq 1$, real $0<\delta \leq 1/8$ and vector $\vec{r}_0 \in \mZ^m$, let $D_1^{(cut)} = D_{\mZ^m,\sigma}^{(cut)} + \vec{r}_0$ and $D_2^{(cut)} = D_{\mZ^m,\sigma}^{(cut)}$ be relatively shifted tail-cut discrete Gaussian distributions, where $D_{\mZ^m,\sigma}^{(cut)}$ is the discrete Gaussian $D_{\mZ^m,\sigma}$ with its tails cut to the support $[-k\sigma,k\sigma]^m$ by rejection sampling. If the conditions
$k \geq 1 + \sqrt{1/\pi \cdot \ln(2m/\delta)}$ and $\sigma/\|\vec{r}_0\|_{\infty} \geq 4 \pi \cdot k \cdot m$ hold, then, for any event $E$ defined over the support of $D_1^{(cut)}$ we have
$$
D_2^{(cut)}(E) \geq \frac{1}{2e^{1+2\delta}} \cdot \left(D_1^{(cut)}(E) - \delta\right).
$$
\end{lemma}

\subsection{Correctness Analysis}
\label{sec:2correctness}

\paragraph{correctness analysis}
We start with the noise sampled during key generation:
\[e_{0} \randomsample \chi_{\alpha q}\]
where \(\chi\) is typically a Gaussian distribution with standard deviation \(\alpha q\). The noise bound of the first level cihertext \(c = (\mathbf{c_{0}, c_{1}})\) would be \(B_{0} = \norminf{[<\mathbf{c,s}>]_{q}}\). We have

\begin{align*}
  \left[ \langle \mathbf{c, s} \rangle \right]_{q} &= \mathbf{p_{0}u} + t \mathbf{g} + \mathbf{m} + \mathbf{p_{1}us} + t \mathbf{fs}\\
  &= m + t(\mathbf{g + fs -e_{0}u})
\end{align*}

So we can approximately bound \(B_{0} \leq t \norminf{e_{0}}^{2}\). The noise of the HD ciphertext (unmask) \(\norminf{e_{HD}}\) can be bounded by \(2nB_{0} + n B_{0}^{2}\), recall that
\[
  enc(HD) = enc_{1}(\mathbf{T})C_{1} + enc_{2}(\mathbf{Q})C_{2} - 2 enc_{1}(\mathbf{T})enc_{2}(\mathbf{Q})
\]
The final masked ciphertext will have noise
\[
\norminf{e_{HDM}} \leq (4\pi kn + 1)n(B_{0}^{2} + 2B_{0})
\]
Where \(k = 1 + \sqrt{\frac{1}{\pi \log{4n FAR^{-1}}}}\). We can derive the final correctness condition:
\[
q > 4 \pi n^{2} t (B_{0}^{2} + 2B_{0})
\]

Notations
\begin{itemize}
\item \(\alpha q\) standard deviation of the original noise distribution, assume Gaussian Distribution \(\chi\)
\item \(e_{0}\) original noise used in key generation and original encryption ($(\mathbf{p_0},\mathbf{p_1})$ where
  $\mathbf{p_1} \randomsample R_Q$ and $\mathbf{p_0} = -(\mathbf{p_1}\mathbf{s} + t\mathbf{e})$ with
  $\mathbf{e} \randomsample \chi$)
\item \(B_{0}\) noise of the first level ciphertext
  \[
    Enc_{pk}(\mathbf{m}) = (\mathbf{c_0},\mathbf{c_1}) = (\mathbf{p_0}\mathbf{u} + t\mathbf{g} + \mathbf{m},
    \mathbf{p_1}\mathbf{u} + t\mathbf{f})
  \]
\end{itemize}
  
 


\section{Results}
\label{sec:secProcResult}


    
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
