\chapter{The Second Protocol - Covering Circuit Privacy}
\label{chap:renyiDivergence}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

\section{Introduction}
\label{sec:secProcIntro}
This chapter discusses Circuit Privacy, which is one important aspect of
Homomorphic Encryption, and how does it affect the security of our first variant
of the protocol. We also describe a new technique being used to improve the
security proof of the protocol, to make sure that we can choose parameters small
enough for the system to be practical but still preserve the security level. In
particular, we show how to use the infinity-order Renyi Divergence (RD) instead of
traditional Statistical Distance in the proof in order to gain significant
improvements in terms of the initial noise bound parameter, which will result in
smaller choice of the moduli \(q\) for the ring \(R_{q}\) being used in the
cryptosystem.

Circuit Privacy means that the ciphertext generated by a homomorphic operation
does not reveal anything about the circuit it evaluates, except for the output
value of the circuit. This property applies even to the party who generated the
keys.

Consider a ciphertext product result in a BV cryptosystem (section
\ref{sec:BVScheme}).
\[
  mult(c,c') = (\mathbf{c_0}\mathbf{c_0'}, \mathbf{c_0}\mathbf{c_1'} +
  \mathbf{c_1}\mathbf{c_0'}, \mathbf{c_1}\mathbf{c_1'})
\]
The noise term of this result ciphertext correlates to both $\mathbf{s}$ and
$\mathbf{m}$.  In many contexts, this leakage might be fine for a genuine user
with a secret key because s/he is supposed to know the key and decrypt the
message.  However, in many other contexts, especially in multi-factor
authentication scenarios, this is not the case. For instance, in our system, we
do not want the client to know the Hamming Distance value, so that an attacker
with a stolen device and a secret key cannot derive information about the data
stored in the server from ?. We propose to mask this leakage by homomorphically
adding the ciphertext $Enc(HD)$ with $Enc(r)$ to refresh the noise terms,
together with masking the Hamming Distance. The question is how much can we
shift the original noise distribution to preserve correctness while providing
the new security measure.

Let $D_1$ and $D_2$ be the probability distributions of the original noise in
the ciphertext $Enc(HD)$ and the new shifted noise of $Enc(HD+r)$. We observe
that the 2 distributions are identical Gaussian distribution with the same
standard deviation $\sigma$ and different means. Let $r_0$ be the shifted in
means of $D_1$ and $D_2$.  Our goal is to set up parameter $r_0$ such that $D_1$
and $D_2$ are computational indistinguishable while keeping other parameters of
the cryptosystem within practical performance thresholds. Statistical Distance
(SD) is normally used to measure the difference of distributions and is
generally bounded by
\[
SD(D_1, D_2) = \frac{1}{2}\sum_{x\in X}|D_1(x) - D_2(x)| \leq K\times \frac{r_0}
{\sigma}
\]
Where $K$ is a constant. We quickly see that in order for $SD$ to be
indistinguishable ($SD < \epsilon \approx \frac{1}{2^\lambda}$, where $\lambda$
is the security parameter), the standard deviation of the initial noise needs to
be really large, which is $\sigma \geq Kr_02^\lambda$.

In the work of \cite{bai2015improved}, the authors proposed Renyi Divergence
(RD) as an alternative to measure distributions closeness and its applications
to security proofs. $R_a(D_1\|D_2)$ of order $a$ between $D_1$ and $D_2$ is
defined as the expected value of $(D_1(x)/D_2(x))^{a-1}$ over the randomness of
$x$ sampled from $D_1$.
\[
R_a(D_1\|D_2) = \left( \sum_{x \in D_1}\frac{D_1(x)^a}{D_2(x)^{a-1}} \right)^
{\frac{1}{a-1}}
\]
Similar to SD, RD is useful in our context with its \textit{Probability
  Preservation} property (we refer readers to \cite{bai2015improved} for
detailed formal descriptions): Given $D_1$ and $D_2$ as described, for any event
$E$, for instance, we want to look at $P(E)$ is the winning probability of the
attacker in the distinguishing game, the probability of the event with respect
to $D_2$ is bounded by
\begin{align}
\label{eq:renyi}
D_2(E) \geq D_1(E)^{\frac{a}{a-1}}/RD_a(D1\|D_2)
\end{align}
Particularly, if we look at the second order ($a = 2$) of RD like previous
works, we would have $ D_2(E) \geq D_1(E)^2/RD_2(D1\|D_2) $.  Provided that the
distribution functions of $D_1$ and $D_2$ are discrete Gaussian on lattices,
which is of the form
$\rho_\sigma(x) = \frac{1}{\sigma}e^{-\pi\frac{x^2} {\sigma^2}}$ , we have
$RD_2(D_1\|D_2) = e^{2\pi \frac{r_0^2}{\sigma^2}} \approx e^{2\pi}$, when $r_0$
is much smaller than $\sigma$. That means when switching from $D_1$ to $D_2$,
the success probability of $E$ will be at least the old probability to the power
of 2 divided by some constant. This squaring factor brings a big trade-off, for
example, in our protocol, we would need to use $FAR = 2^{-20}$ in the
non-privacy biometric settings to get $FAR=2^{-10}$ in our scheme.

We aim at a solution to remove this factor. The idea is to look at $RD_\infty$
in stead of $RD_2$: from equation (\ref{eq:renyi}), we can see that when $a$ is
large, $\frac{a}{a-1}$ becomes 1. However, for usual Gaussian distributions,
$RD_\infty$ is also infinity (not a constant $e^{2\pi}$ like in $RD_2$ when
$a=2$). This is due to the ratios $\frac{D_1(x)}{D_2(x)}$ become large when
samplings are done in the extreme tails of the distributions. Our idea is to
truncate the distribution when doing noise sampling: if we get a noise value
that is too far in the tail, we reject and sample again. As a result, the
truncated distribution grows slightly, that means, the small noises have a bit
higher probability when sampling, which does not have a big impact in security.


\section{Context and Related Works}
\label{sec:secProcPrevious}

\subsection{Definitions}
\label{sec:renyiDefinition}

\subsubsection{Circuit Privacy}
\label{sec:renyiCircuitPrivacy}

To define Circuit Privacy, we view the operation of \(Evaluate\) (Section
\ref{sec:defHomo}) as a protocol between a client who generates the keys and
encrypts the input, and a server who evaluates some function on that input and
returns the result to the client.

\begin{definition}
  [Circuit privacy.] A homomorphic encryption scheme E = (KeyGen,
  Encrypt, Decrypt, Evaluate), correct for the circuit family C, is circuit
  private for C, if there exists an efficient simulator Sim such that, for every
  \(\tau \in \mathbb{N}, \pi \in C_{\tau},\) and plaintext bits \textbf{b} =
  \((b_{1}, \dots, b_{t}) \in \{0,1\}^{t}\), one for every input bit of \(\pi\),
  we have\\
  $$Real_{\pi,\mathbf{b}}(\lambda) \approx Sim(1^{\lambda}, 1^{\tau},
  \mathbf{b}, \pi(\mathbf{b}))$$ Where\\
  \(Real_{\pi,\mathbf{b}}(\lambda) = \{(r,r',c): r, r' \randomsample \$, (sk,
  pk) \gets KeyGen(1^{\lambda}, 1^{\tau}, r), c \gets Encrypt(pk, \mathbb{b},
  r'), c' \gets Evaluate(pk, \pi, \mathbf{c})\}\)
\end{definition}
We note that the simulator \(Sim\) is given the output \(\pi(\mathbf{b})\) but
not the description of the circuit \(\pi\) itself, and it needs to simulate the
view that includes all the randomness from key generation, encryption and
ciphertext evaluation operations

\subsubsection{Renyi Divergence}
\label{sec:renyisubdefinition}
\begin{definition}
  [Renyi Divergence] Let \(Supp(D) = \{x: D(x) \neq 0\}\) denotes the
  \textit{support} for a probability distribution \(D\). For any two discrete
  probability distributions \(P\) and \(Q\) such that
  \(Supp(P) \subseteq Supp(Q)\) and \(a \in (1, +\infty)\), the Renyi divergence
  of order a is defined by\\
  $$RD_{a} = \left(\sum_{x \in Supp(p)}\frac{P(x)^{a}}{Q(x)^{a-1}}\right)^{\frac{1}{a-1}}$$
\end{definition}
When \(a = 2\), the subscript is normally omitted, the Renyi divergences of
order \(+\infty\) is defined by
\(RD_{\infty} = max_{x \in Supp(P)}\frac{P(x)}{Q(x)}\). The definitions are
extended in the natural way to continuous distributions. The following
properties are considered analogues of those of Statistical Distance. Proofs can
be found in \cite{langlois2014gghlite}.
\begin{lemma}
  Let \(a \in [1, +\infty]\). Let \(P\) and \(Q\) denote distributions with
  \(Supp(P) \subseteq Supp(Q)\). Then the following properties hold:
  \begin{description}
  \item [Log. Positivity.] \(RD_{a}(P || Q) \geq RD_{a}(P||P) = 1\).
  \item[Data Processing Inequality]
    \(RD_{a}(P^{f} || Q^{f}) \leq RD_{a}(P || Q)\) for any function \(f\), where
    \(P^{f}\) and \(Q^{f}\) denote the distribution of \(f(y)\) induced by
    sampling \(y \randomsample P\) and \(y \randomsample Q\)
  \item[Multiplicative] Assume \(P\) and \(Q\) are two distributions of a pair
    of random variables \((Y_{1}, Y_{2})\). For \(i \in \{1,2\}\), let \(P_{i}\)
    and \(Q_{i}\) denote the marginal distribution of \(Y_{i}\) under \(P\) and
    \(Q\), and let \(P_{2|1}(\cdot | y_{1})\) and \(Q_{2|1}(\cdot | y_{1})\)
    denote the conditional distribution of \(Y_{2}\) given that
    \(Y_{1} = y_{1}\). Then we have:
    \begin{itemize}
    \item \(RD_{a}(P||Q) = RD_{a}(P_{1}||Q_{1}) \cdot RD_{a}(P_{2}||Q_{2}))\) if
    \(Y_{1}\) and \(Y_{2}\) are independent for \(a \in [1,\infty]\).
    \item
    \(RD_{a}(P||Q) \leq RD_{\infty}(P_{1} || Q_{1}) \cdot max_{y_{1} \in X}
    RD_{a}(P_{2|1}(\cdot | y_{1})||Q_{2|1}(\cdot | y_{1}))\)
    \end{itemize}
  \item[Probability Preservation] Let \(E \subseteq Supp(Q)\) be an arbitrary
    event. If \(a \in (1, +\infty)\), then
    \(Q(E) \geq P(E)^{\frac{a}{a-1}}/RD_{a}(P||Q)\) and
    \(Q(E) \geq P(E)/RD_{\infty}(P||Q)\)
  \item[Weak Triangle Inequality] Let \(P_{1}, P_{2}, P_{3}\) be three
    distributions with
    \(Supp(P_{1}) \subseteq Supp(P_{2}) \subseteq Supp(P_{3})\). Then we have\\
    \(RD_{a}(P_{1}||P_{3}) \leq
    \begin{cases}
      RD_{a}(P_{1}||P_{2}) \cdot R_{\infty}(P_{2}||P_{3}),\\
      RD_{\infty}(P_{1}||P_{2})^{\frac{a}{a-1}} \cdot RD_{a}(P_{2}||P_{3})
      \text{if } a \in (1,+\infty)
    \end{cases}
\)
  \end{description}
\end{lemma}

\subsection{Related Works}
\label{sec:renyiRelatedWorks}
Ling et al. \cite{ling2017hardness} used RD as the framework in distinguishing
problems in the \textit{k}-LWE context, which is a variant of LWE in which the
adversary is given extra information in the distinguishing attack game). The
work of \cite{poppelmann2014enhanced27} used RD order 1 (Kullback-Leibler
divergence) to improve the communication size requirement of BLISS
\cite{ducas2013lattice11}. \cite{bai2015improved5} proposed how generic RD can
be used as an alternative to the statistical distance in proofs for
lattice-based cryptography.  Bogdanove et al. \cite{bogdanov2016hardness4}
adapted the work of \cite{bai2015improved5} to the Learning With Rounding
problem. RD was used in \cite{libert2016signature} in the context of dynamic
group signatures and also in \cite{alkim2016post} to replace LWE noise
distribution by an easier to sample distribution.


\section{Renyi Divergence Analysis technique}
\label{sec:secProcRenyi}
% \subsection{Renyi Divergence and its application in noise masking}
% \label{sec:Renyi_original}

For the following analysis, let $D_1$ to be a discrete Gaussian on $\mZ$ with
deviation parameter $\sigma$ shifted by the constant $r_0 \in \mZ$, while $D_2$
is a discrete Gaussian on $\mZ$ with dev. par. $\sigma$ centered on zero,
i.e. $D_1 = D_{\mZ,\sigma} + r_0$ and $D_2 = D_{\mZ,\sigma}$, where
$D_{\mZ,\sigma}(x) = e^{-\pi \cdot x^2/\sigma^2}/\sum_{z \in \mZ} e^{-\pi \cdot
  z^2/\sigma^2}$ for $x \in \mZ$. To allow us to use $RD_{\infty}$ we use
tail-cut variants $D_1^{(cut)}$ and $D_2^{(cut)}$ of $D_1$ and $D_2$,
respectively with parameter $k$. The parameter $k$ defines where $D_1$ and $D_2$
are cut at, for example, we can set $k=3$ to cut the distributions at 3
deviation parameters from the mean. So, we let $D_{\mZ,\sigma}^{(cut)}$ denote
distribution $D_{\mZ,\sigma}$ tail-cutted to the interval
$[-k \cdot \sigma, k \cdot \sigma]$ by rejection sampling. We let
$D_1^{(cut)} = D_{\mZ,\sigma}^{(cut)} + r_0$ and
$D_2^{(cut)} = D_{\mZ,\sigma}^{(cut)}$. Notice that the supports of
$D_1^{(cut)}$ and $D_2^{(cut)}$ are different, namely
$Supp(D_1^{(cut)}) = [-k\sigma+r_0,k\sigma+r_0]$ while
$Supp(D_2^{(cut)}) = [-k\sigma,k\sigma]$. We assume, without loss of generality,
that $r_0 > 0$. We would like to switch from distribution $D_1^{(cut)}$ to
$D_2^{(cut)}$, but unfortunately $R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)})$
is not finite since $Supp(D_1^{(cut)})$ is not a subset of
$Supp(D_2^{(cut)})$. To satisfy the latter condition, we first switch from
$D_1^{(cut)}$ to $\overline{D}_1^{(cut)}$ by further cutting (by rejection
sampling) the positive tail of $D_1^{(cut)}$ to ensure it does not go beyond the
$k \sigma$ upper bound on tail of $D_2^{(cut)}$, and use a (mild condition)
statistical distance step to lower bound $\overline{D}_1^{(cut)}(E)$. Then, in a
second step using
$Supp(\overline{D}_1^{(cut)})=[-k\sigma+r_0, k\sigma] \subseteq
[-k\sigma,k\sigma] = Supp(D_2^{(cut)})$, we derive a finite upper bound on
$R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)})$ to lower bound
$D_2^{(cut)}(E)$. Details follow.
\begin{sloppypar}
  \textbf{First SD Step.} Since $Supp(D_1^{(cut)})$ is transformed into
  $\overline{D}_1^{(cut)}$ by rejection and resampling if a sample of
  $Supp(D_1^{(cut)})$ falls in $(k\sigma,k\sigma+r_0]$, we have
  $SD(D_1^{(cut)}, \overline{D}_1^{(cut)}) \leq
  D_1^{(cut)}((k\sigma,k\sigma+r_0]) = D_2^{(cut)}((k\sigma-r_0,k\sigma]) =
  D_{\mZ,\sigma}((k\sigma-r_0,k\sigma])/C_2$, where
  $C_2 = D_{\mZ,\sigma}([-k\sigma,k\sigma])$. Now, we have
$$
\Delta \defeq \frac{D_{\mZ,\sigma}((k\sigma-r_0,k\sigma])}{C_2} = \frac{\sum_{z
    \in (k\sigma-r_0,k\sigma])} \e^{-\pi z^2/\sigma^2}}{\sum_{z \in
    [-k\sigma,k\sigma])} \e^{-\pi z^2/\sigma^2}}.
$$
For the numerator, we have an upper bound
$\sum_{z \in (k\sigma-r_0,k\sigma])} \e^{-\pi z^2/\sigma^2} \leq
\int^{\infty}_{k\sigma-r_0} \e^{-\pi z^2/\sigma^2} dz \leq \sigma \cdot \e^{-\pi
  (k\sigma-r_0)^2/\sigma^2}$, using the standard normal distribution upper bound
$\int^{\infty}_{\gamma} \frac{1}{\sqrt{2\pi} \sigma} \cdot \e^{- z^2/\sigma^2}
dz \leq \e^{-\gamma^2/(2\sigma^2)}$ for $\gamma \geq 0$. For the denominator, we
have a lower bound
$\sum_{z \in [-k\sigma,k\sigma]} \e^{-\pi z^2/\sigma^2} \geq 2 \cdot \sum_{z \in
  [0,k\sigma])} \e^{-\pi z^2/\sigma^2} \geq 2 \cdot (\int^{\infty}_{0} \e^{-\pi
  z^2/\sigma^2} dz - \int^{\infty}_{k\sigma} \e^{-\pi z^2/\sigma^2} dz) \geq
\sigma \cdot (1 - 2 \cdot \e^{-\pi (k\sigma-r_0)^2/\sigma^2})$. Therefore,
$SD(D_1^{(cut)}, \overline{D}_1^{(cut)}) \leq \Delta \leq \delta' / (1-2\delta')
\leq 2\delta'$ if $\delta' \leq 1/4$, where
$\delta' = \e^{-\pi (k\sigma-r_0)^2/\sigma^2}$. Defining $\delta = 2\delta'$, we
have $\Delta \leq \delta$ if $\delta \leq 1/8$ and the conditions
$r_0 \leq \sigma$ and $k \geq 1 + \sqrt{1/\pi \cdot \ln(2/\delta)}$
hold. Therefore, for any event $E$ we have
$\overline{D}_1^{(cut)}(E) \geq D_1^{(cut)}(E) - \delta$.
\end{sloppypar}
% with probability $D_1^{(cut)}(E)$ \geq \varepsilon$, we have can take $\delta
% = \var{\epsilon This tells us how to set the tail cut parameter $k$.

\textbf{Second RD step.} The desired RD of order $\infty$ is defined by
\[
  R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)})) = \max_{x \in
    [-k\sigma+r_0,k\sigma]} \frac{\overline{D}_1^{(cut)}} {D_2^{(cut)}(x)}.
\]
Observe that, for each $x \in [-k\sigma+r_0,k\sigma]$, we have
$\overline{D}_1^{(cut)}(x) = C \cdot D_1^{(cut)}(x)$, where the normalization
constant
$C = \frac{1}{1-D_1^{(cut)}((k\sigma,k\sigma+r_0])} =
\frac{1}{1-D_2^{(cut)}((-k\sigma+r_0,k\sigma])} = \frac{1}{1-\Delta}$, where
$\Delta$ is defined and upper bounded by $\delta$ above under the assumed
conditions on $k$ and $\delta$. Since the $D_1^{(cut)}(x)$ and $D_2^{(cut)}(x)$
are shifts of each other, they have the same rejection sampling normalization
constant with respect to $D_1$ (resp. $D_2$). Therefore,
$D_1^{(cut)}(x)/D_2^{(cut)}(x)=D_1(x)/D_2(x)$ for each $x$ in the support of
both $D_1^{(cut)}$ and $D_2^{(cut)}$, and we have
% $D_1(x)$ and $D_2(x)$ divided by some constant factor $C_1$ and $C_2$ (to keep
% the cumulative probability area to be 1) . In our context, as $D_1(x)$ and
% $D_2(x)$ are the same distributions with different means, so $C_1 = C_2$.
\begin{align*}
  R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)}) &\leq \frac{1}{1-\delta} \cdot \max_{x \in [-k\sigma+r_0,k\sigma]} \frac{D_1(x)}
                                                  {D_2(x)}\\
                                                &= \max_{x \in [-k\sigma,k\sigma]}\frac{e^{\frac{-\pi(x-r_0)^2}{\sigma^2}}}{e^{-\pi\frac{x^2}{\sigma^2}}}
  \\
                                                &= e^{\pi \cdot r_0^2/\sigma^2} \cdot \max_{x \in [-k\sigma+r_0,k\sigma]}e^{\frac{2\pi r_0}
                                                  {\sigma^2}x}
\end{align*}
This is an exponential function and we get the max value at $x = k\sigma$:
$$
R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)}) = e^{1/(1-\delta)} \cdot e^{\pi
  \cdot r_0^2/\sigma^2 + 2\pi k \cdot r_0/\sigma}.
$$
Since $0<\delta \leq 1/8$, the first factor above is
$\leq 1+2\delta \leq e^{2\delta}$. Also, a simple computation shows that the
second factor is $\leq e$ if the condition $\sigma/r_0 \geq 4 \pi \cdot k$ is
satisfied using $k \geq 1$. We conclude, under the assumed parameter conditions
that
$R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)})) \leq e^{1+2\delta} = c'(\delta)$
is constant for constant $\delta>0$, so that, by the RD probability preservation
property
$D_2^{(cut)}(E) \geq \frac{1}{c(\delta)} \cdot \overline{D}_1^{(cut)}(E) \geq
\frac{1}{c(\delta)} \cdot (D_1^{(cut)}(E) - \delta)$. Note that if
$D_1^{(cut)}(E)=\varepsilon$, then by choosing $\delta = \varepsilon/2$, we get
$D_2^{(cut)}(E) \geq \frac{1}{2c(\delta)} \cdot \varepsilon$, and we only need
$k \geq 1 + \sqrt{1/\pi \cdot \ln(2/\delta)}$ and $\sigma/r_0$ logarithmic in
$1/\delta$, much smaller than $\sigma/r_0$ linear in $1/\delta$, which we would
need if we were to use the `SD only' analysis approach.

The above discussion immediately generalizes from the one-dimensional case of
discrete Gaussian samples over $\mZ$ to the $m$-dimensional case of discrete
Gaussian samples over $\mZ^m$ due to the independence of the $m$
coordinates. The only changes to the above argument is that the statistical
distance in the `SD step' can multiply by at most a factor $m$, whereas the RD
in the `RD step' above gets raised to the $m$'th power, where we replace $r_0$
by $\|\vec{r}_0\|_{\infty}$. We compensate for this by replacing the bound
$\delta$ on $\Delta$ in the above analysis by the bound $\delta/m$. We have
therefore proved the following result used in our impersonation security proof,
which improved upon the $R_2$-based analogue result for shifted Gaussians stated
in~\cite{langlois2014gghlite}.

% \keq Note that when $r_0$ is much smaller than $\sigma$, the first term
% $e^{\frac{\pi r_0^2}{\sigma^2}} \approx 1$. However, if $r_0$ is too small
% compared to $\sigma$, then the second term is at most $e^{2k\pi}$, which is
% also very large in the relation of equation (\ref{eq:renyi}). Or, we can also
% choose parameters such as $k = 3$ and $\frac{r_0}{\sigma} = \frac{1}{6}$, then
% the trade-off factor becomes much smaller, $e^{\pi}$. In conclusion, by
% applying infinity order RD to measure the truncated distributions closeness,
% we might obtain better parameters ($\sigma \geq 6r_0$, for example, compared
% to $\sigma \geq Kr_02^\lambda$ in SD approach) while providing
% \textit{Probability Preservation}, or security against distinguishing
% adversaries.

\begin{lemma} \label{le:Renyi} For integer $m \geq 1$, real
  $\sigma>0$,$k \geq 1$, real $0<\delta \leq 1/8$ and vector
  $\vec{r}_0 \in \mZ^m$, let
  $D_1^{(cut)} = D_{\mZ^m,\sigma}^{(cut)} + \vec{r}_0$ and
  $D_2^{(cut)} = D_{\mZ^m,\sigma}^{(cut)}$ be relatively shifted tail-cut
  discrete Gaussian distributions, where $D_{\mZ^m,\sigma}^{(cut)}$ is the
  discrete Gaussian $D_{\mZ^m,\sigma}$ with its tails cut to the support
  $[-k\sigma,k\sigma]^m$ by rejection sampling. If the conditions
  $k \geq 1 + \sqrt{1/\pi \cdot \ln(2m/\delta)}$ and
  $\sigma/\|\vec{r}_0\|_{\infty} \geq 4 \pi \cdot k \cdot m$ hold, then, for any
  event $E$ defined over the support of $D_1^{(cut)}$ we have
$$
D_2^{(cut)}(E) \geq \frac{1}{2e^{1+2\delta}} \cdot \left(D_1^{(cut)}(E) - \delta\right).
$$
\end{lemma}

\subsection{Correctness Analysis}
\label{sec:2correctness}

\paragraph{correctness analysis}
We start with the noise sampled during key generation:
\[e_{0} \randomsample \chi_{\alpha q}\] where \(\chi\) is typically a Gaussian
distribution with standard deviation \(\alpha q\). The noise bound of the first
level cihertext \(c = (\mathbf{c_{0}, c_{1}})\) would be
\(B_{0} = \norminf{[<\mathbf{c,s}>]_{q}}\). We have

\begin{align*}
  \left[ \langle \mathbf{c, s} \rangle \right]_{q} &= \mathbf{p_{0}u} + t \mathbf{g} + \mathbf{m} + \mathbf{p_{1}us} + t \mathbf{fs}\\
                                                   &= m + t(\mathbf{g + fs -e_{0}u})
\end{align*}

So we can approximately bound \(B_{0} \leq t \norminf{e_{0}}^{2}\). The noise of
the HD ciphertext (unmask) \(\norminf{e_{HD}}\) can be bounded by
\(2nB_{0} + n B_{0}^{2}\), recall that
\[
  enc(HD) = enc_{1}(\mathbf{T})C_{1} + enc_{2}(\mathbf{Q})C_{2} - 2
  enc_{1}(\mathbf{T})enc_{2}(\mathbf{Q})
\]
The final masked ciphertext will have noise
\[
\norminf{e_{HDM}} \leq (4\pi kn + 1)n(B_{0}^{2} + 2B_{0})
\]
Where \(k = 1 + \sqrt{\frac{1}{\pi \log{4n FAR^{-1}}}}\). We can derive the
final correctness condition:
\[
q > 4 \pi n^{2} t (B_{0}^{2} + 2B_{0})
\]

Notations
\begin{itemize}
\item \(\alpha q\) standard deviation of the original noise distribution, assume
  Gaussian Distribution \(\chi\)
\item \(e_{0}\) original noise used in key generation and original encryption
  ($(\mathbf{p_0},\mathbf{p_1})$ where $\mathbf{p_1} \randomsample R_Q$ and
  $\mathbf{p_0} = -(\mathbf{p_1}\mathbf{s} + t\mathbf{e})$ with
  $\mathbf{e} \randomsample \chi$)
\item \(B_{0}\) noise of the first level ciphertext
  \[
    Enc_{pk}(\mathbf{m}) = (\mathbf{c_0},\mathbf{c_1}) = (\mathbf{p_0}\mathbf{u}
    + t\mathbf{g} + \mathbf{m}, \mathbf{p_1}\mathbf{u} + t\mathbf{f})
  \]
\end{itemize}
  
 


\section{Results}
\label{sec:secProcResult}


    
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
