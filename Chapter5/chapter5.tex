\chapter{The Second Protocol - Covering Circuit Privacy}
\label{chap:renyiDivergence}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

\section{Introduction}
\label{sec:secProcIntro}
This chapter discusses Circuit Privacy, which is an important aspect of
Homomorphic Encryption, and how it affects the security of our first variant
of the protocol. We also describe a new technique being used to improve the
security proof of the protocol, to make sure that we can choose parameters small
enough for the system for practical reasons, but still preserve the security level. In
particular, we show how to use the infinity-order Renyi Divergence (RD) instead of
traditional Statistical Distance in the proof, in order to significantly lower the initial noise bound parameter, which will result in a smaller moduli \(q\) for the ring \(R_{q}\) used by the
cryptosystem.

Circuit Privacy means that the ciphertext generated by a homomorphic operation
does not reveal anything about the circuit it evaluates, except its output
value. This property applies even to the party having generated the
keys.

Consider a ciphertext product result in a BV cryptosystem (section
\ref{sec:BVScheme}).
\[
  mult(c,c') = (\mathbf{c_0}\mathbf{c_0'}, \mathbf{c_0}\mathbf{c_1'} +
  \mathbf{c_1}\mathbf{c_0'}, \mathbf{c_1}\mathbf{c_1'})
\]
The noise term of this result ciphertext correlates to both $\mathbf{s}$ and $\mathbf{m}$.
In many contexts, a leakage of this kind might be inconsequential for a genuine user with a secret key, because s/he is supposed to know the key and decrypt the message.
However, in many other contexts, especially in multi-factor authentication
scenarios, this is not the case. For instance, in our system, we do not want the
client to know the Hamming Distance value, so that an attacker with a stolen
device and a secret key cannot derive information about the data stored in the
server from the enrolment stage?. We propose to mask this data by homomorphically adding the ciphertext
$Enc(HD)$ to $Enc(r)$ so as to refresh the noise terms while masking the Hamming Distance at the same time. The question is how far from the original noise distribution we can move away without compromising correctness, if this new security measure is applied.

Let $D_1$ and $D_2$ be the probability distributions of the original noise in the ciphertext $Enc(HD)$ and the new shifted noise of $Enc(HD+r)$. We observe that the
2 Gaussian distributions are identical with the same standard deviation
$\sigma$ and different means. Let $r_0$ be the shift in the means of $D_1$ and $D_2$.
Our goal is to set up parameter $r_0$, such that $D_1$ and $D_2$ are computationally
indistinguishable while keeping other parameters of the cryptosystem within practical
performance
thresholds. Statistical Distance (SD) is normally used to measure the difference of distributions and is generally bound by
\[
SD(D_1, D_2) = \frac{1}{2}\sum_{x\in X}|D_1(x) - D_2(x)| \leq K\times \frac{r_0}
{\sigma}
\]
where $K$ is a constant. We quickly see that in order for $SD$ to be indistinguishable, ($SD < \epsilon \approx \frac{1}{2^\lambda}$, where $\lambda$ is
the security parameter), the standard deviation of the initial noise needs to be really large, that is $\sigma \geq Kr_02^\lambda$.

In the work of \cite{bai2015improved}, the authors proposed Renyi Divergence (RD) as
an alternative to measure distributions' closeness and its applications to security proofs. $R_a(D_1\|D_2)$ of order $a$
between $D_1$ and $D_2$ is defined as the expected value of $(D_1(x)/D_2(x))^{a-1}$ over the randomness of $x$, sampled from $D_1$.
\[
R_a(D_1\|D_2) = \left( \sum_{x \in D_1}\frac{D_1(x)^a}{D_2(x)^{a-1}} \right)^
{\frac{1}{a-1}}
\]
Similar to SD, RD is useful in our context with its \textit{Probability Preservation} property (we refer readers to \cite{bai2015improved} for detailed formal
descriptions): Given $D_1$ and $D_2$ as described, for any event $E$, for instance, we want to verify that $P(E)$ is the winning probability of the attacker in the distinguishing game, the probability of the event with respect to $D_2$ being bound by
\begin{align}
\label{eq:renyi}
D_2(E) \geq D_1(E)^{\frac{a}{a-1}}/RD_a(D1\|D_2)
\end{align}
Particularly, if we look at the second order ($a = 2$) of RD as done in previous
works, we would have $ D_2(E) \geq D_1(E)^2/RD_2(D1\|D_2) $.  Provided that the
distribution functions on lattices of $D_1$ and $D_2$ are discrete Gaussian,
which is of the form
$\rho_\sigma(x) = \frac{1}{\sigma}e^{-\pi\frac{x^2} {\sigma^2}}$ ,
$RD_2(D_1\|D_2) = e^{2\pi \frac{r_0^2}{\sigma^2}} \approx e^{2\pi}$, when $r_0$
is much smaller than $\sigma$. This means that when switching from $D_1$ to $D_2$,
the success probability of $E$ will be at least the old probability to the power
of 2 divided by some constant. This squaring factor brings about a big trade-off: for
example, in our protocol, we would need to use $FAR = 2^{-20}$ in the
non-privacy biometric settings to get $FAR=2^{-10}$ into our scheme.

We aim at a solution to remove this factor. The idea is to look at $RD_\infty$
instead of at $RD_2$: from equation (\ref{eq:renyi}), we can infer that when $a$ is
large, $\frac{a}{a-1}$ becomes 1. However, for usual Gaussian distributions, the value of
$RD_\infty$ is also infinity (not a constant $e^{2\pi}$ like in $RD_2$ when
$a=2$). This is due to the ratios $\frac{D_1(x)}{D_2(x)}$, which become large when
samplings are done in the extreme tails of the distributions. Our idea is to
truncate the distribution when doing noise sampling: if we get a noise value
that is too close to the tail, we reject it and sample again. As a result, the
truncated distribution grows slightly, that means, the small noises show
higher probability when sampling, which does not have a high impact on security.


\section{Context and Related Work}
\label{sec:secProcPrevious}

\subsection{Definitions}
\label{sec:renyiDefinition}

\subsubsection{Circuit Privacy}
\label{sec:renyiCircuitPrivacy}

To define Circuit Privacy, we view the operation of \(Evaluate\) (Section
\ref{sec:defHomo}) as a protocol between a client who generates the keys and
encrypts the input, and a server who evaluates some function on that input and
returns the result to the client.

\begin{definition}
  [Circuit privacy.] A homomorphic encryption scheme E = (KeyGen,
  Encrypt, Decrypt, Evaluate), correct for the circuit family C, is circuit
  private for C, if there exists an efficient simulator Sim such that, for every
  \(\tau \in \mathbb{N}, \pi \in C_{\tau},\) and plaintext bits \textbf{b} =
  \((b_{1}, \dots, b_{t}) \in \{0,1\}^{t}\), one for every input bit of \(\pi\),
  we have\\
  $$Real_{\pi,\mathbf{b}}(\lambda) \approx Sim(1^{\lambda}, 1^{\tau},
  \mathbf{b}, \pi(\mathbf{b}))$$ Where\\
  \(Real_{\pi,\mathbf{b}}(\lambda) = \{(r,r',c): r, r' \randomsample \$, (sk,
  pk) \gets KeyGen(1^{\lambda}, 1^{\tau}, r), c \gets Encrypt(pk, \mathbb{b},
  r'), c' \gets Evaluate(pk, \pi, \mathbf{c})\}\)
\end{definition}
We note that the simulator \(Sim\) is given the output \(\pi(\mathbf{b})\) but
not the description of the circuit \(\pi\) itself, and it needs to simulate the
view that includes the randomness from key generation, encryption and
ciphertext evaluation operations

\subsubsection{Renyi Divergence}
\label{sec:renyisubdefinition}
\begin{definition}
  [Renyi Divergence] Let \(Supp(D) = \{x: D(x) \neq 0\}\) denotes the
  \textit{support} for a probability distribution \(D\). For any two discrete
  probability distributions \(P\) and \(Q\) such that
  \(Supp(P) \subseteq Supp(Q)\) and \(a \in (1, +\infty)\), the Renyi divergence
  of order a is defined by\\
  $$RD_{a} = \left(\sum_{x \in Supp(p)}\frac{P(x)^{a}}{Q(x)^{a-1}}\right)^{\frac{1}{a-1}}$$
\end{definition}
When \(a = 2\), the subscript is normally omitted, the Renyi divergence of
order \(+\infty\) is defined by
\(RD_{\infty} = max_{x \in Supp(P)}\frac{P(x)}{Q(x)}\). The definitions are
extended in the natural way to continuous distributions. The following
properties are considered analogues of those of Statistical Distance. Proofs can
be found in \cite{langlois2014gghlite}.
\begin{lemma}
  Let \(a \in [1, +\infty]\). Let \(P\) and \(Q\) denote distributions with
  \(Supp(P) \subseteq Supp(Q)\). Then the following properties hold:
  \begin{description}
  \item [Log. Positivity.] \(RD_{a}(P || Q) \geq RD_{a}(P||P) = 1\).
  \item[Data Processing Inequality]
    \(RD_{a}(P^{f} || Q^{f}) \leq RD_{a}(P || Q)\) for any function \(f\), where
    \(P^{f}\) and \(Q^{f}\) denote the distribution of \(f(y)\) induced by
    sampling \(y \randomsample P\) and \(y \randomsample Q\)
  \item[Multiplicative] Assume \(P\) and \(Q\) are two distributions of a pair
    of random variables \((Y_{1}, Y_{2})\). For \(i \in \{1,2\}\), let \(P_{i}\)
    and \(Q_{i}\) denote the marginal distribution of \(Y_{i}\) under \(P\) and
    \(Q\), and let \(P_{2|1}(\cdot | y_{1})\) and \(Q_{2|1}(\cdot | y_{1})\)
    denote the conditional distribution of \(Y_{2}\) given that
    \(Y_{1} = y_{1}\). Consequently:
    \begin{itemize}
    \item \(RD_{a}(P||Q) = RD_{a}(P_{1}||Q_{1}) \cdot RD_{a}(P_{2}||Q_{2}))\) if
    \(Y_{1}\) and \(Y_{2}\) are independent for \(a \in [1,\infty]\).
    \item
    \(RD_{a}(P||Q) \leq RD_{\infty}(P_{1} || Q_{1}) \cdot max_{y_{1} \in X}
    RD_{a}(P_{2|1}(\cdot | y_{1})||Q_{2|1}(\cdot | y_{1}))\)
    \end{itemize}
  \item[Probability Preservation] Let \(E \subseteq Supp(Q)\) be an arbitrary
    event. If \(a \in (1, +\infty)\), then
    \(Q(E) \geq P(E)^{\frac{a}{a-1}}/RD_{a}(P||Q)\) and
    \(Q(E) \geq P(E)/RD_{\infty}(P||Q)\)
  \item[Weak Triangle Inequality] Let \(P_{1}, P_{2}, P_{3}\) be three
    distributions with
    \(Supp(P_{1}) \subseteq Supp(P_{2}) \subseteq Supp(P_{3})\). Consequently:\\
    \(RD_{a}(P_{1}||P_{3}) \leq
    \begin{cases}
      RD_{a}(P_{1}||P_{2}) \cdot R_{\infty}(P_{2}||P_{3}),\\
      RD_{\infty}(P_{1}||P_{2})^{\frac{a}{a-1}} \cdot RD_{a}(P_{2}||P_{3})
      \text{if } a \in (1,+\infty)
    \end{cases}
\)
  \end{description}
\end{lemma}

\subsection{Related Work}
\label{sec:renyiRelatedWorks}
Ling et al. \cite{ling2017hardness} used RD as the framework for distinguishing
problems in the \textit{k}-LWE context, which is a variant of LWE in which the
adversary is given extra information in the distinguishing attack game. The
work of \cite{poppelmann2014enhanced27} used RD order 1 (Kullback-Leibler
divergence) to improve the communication size requirement of BLISS
\cite{ducas2013lattice11}. \cite{bai2015improved5} showed how generic RD can
be used as an alternative to the statistical distance in proofs for
lattice-based cryptography.  Bogdanove et al. \cite{bogdanov2016hardness4}
adapted the work of \cite{bai2015improved5} to the Learning With Rounding
problem. RD was used in \cite{libert2016signature} in the context of dynamic
group signatures and also in \cite{alkim2016post} to replace LWE noise
distribution by an easier to sample distribution.



\section{Our Protocol}
\label{sec:our_protocol}
We denote $\user$ to be the client and $\server$ to be the server. There are 3 main submodules in the protocol: Setup,
Enrol, and Authenticate.
\begin{description}
\item[Setup.] $\user$ and $\server$ initialize the parameters, taking into account the following categories:
  \begin{description}
  \item[Biometric Authentication System Parameters.] These parameters are standard ones used by non privacy preserving
    biometric authentication systems:
    \begin{itemize}
    \item False Acceptance Rate (FAR) and False Rejection Rate (FRR)
    \item $a$: The maximum number of incorrect authentication attempts allowed by $\server$.
    \item $\tau$: Threshold to compare the Hamming Distance to decide the authentication result.
      % \item $B$: An integer defines the base of the HD in a
      %   decomposition operation.
    \item \(n'\): The bit-length of the encoded
      biometric data.
    \end{itemize}
  \item[Ring-LWE based techniques parameters.] These parameters are used in the lattice-based cryptosystem which provides
    client privacy against long term quantum attacks.
    \begin{itemize}
    \item $\lambda$: General security parameter of the cryptosystem (the adversary's winning chance in the CPA security game is \(1/2^{\lambda}\))
    \item $n$: Integer $n$ defining the plaintext and ciphertext rings. This will be refered to as the degree of
      the polynomial objects or dimension of the underlying lattice during correctness and security proofs.
    \item $t$: Integer $t$ defining the plaintext space ring $R_t = \mathbb{Z}_t[x]/x^n+1$.
    \item $q$: Integer $q$ defining the ciphertext space ring $R_q = \mathbb{Z}_q[x]/x^n+1$
    \item $\chi_{\alpha q}$: A distribution used to sample noises for LWE-based techniques.  Typically, $\chi$
      is a Gaussian distribution with standard deviation $\alpha q$.
    \item \(\delta\): Renyi Divergence parameter for the security of noise masking.
    \end{itemize}
  \item[Keygen.] Keys are generated for $\user$:
    \begin{itemize}
    \item Secret key: $\mathbf{s} \randomsample \chi_{\alpha q}^n$, \(sk = (1, \mathbf{s, s^{2}, ...})\)
    \item Public key: $pk = \mathbf{(p_0,p_1)}$, with $\mathbf{p_1} \randomsample R_q$ and
      $\mathbf{p_0} = -\mathbf{p_1s} - t\mathbf{e}$, with $\mathbf{e} \randomsample \chi_{\alpha q}^n$.
    \end{itemize}
  \end{description}
\item [Enrolment.] $\user$ extracts the biometric template $\mathbf{x}$, the bit string $\mathbf{x}$ being
  represented as a ring element of ${R}_t$.  The encryption is done by $\enc{\mathbf{x}} = (\mathbf{c_0},\mathbf{c_1})$
  and sent to $\server$.
\item [Authentication.] The following steps are required:
  \begin{enumerate}
  \item $\user$ extracts his biometric features again $\mathbf{y}$ to use them as the query. $\user$ sends
    $\enc{\mathbf{y}} = (\mathbf{c_0', c_1'})$ to $\server$.
  \item ZKP for the first relation: $\user$ has to prove that $\enc{\mathbf{y}}$ is a valid encryption, that is, it
    encrypts a bit string under the BV cryptosystem using the corresponding secret key. This is done by module
    \textbf{SternBV(A, y, x)} described in Sect. \ref{sec:Stern-basedZKP}, where
    $\mathbf{A} = \begin{bmatrix} c_{y0}, t, 0, 1\\p_{y0}, 0, t, 0
    \end{bmatrix}$, \(\mathbf{X} = [\vec{s},\tilde{e_{y}},\vec{e_{0}},y]^T\) and \(\mathbf{Y
    = [c_{y1},p_{y1}]}^{T}\).

  \item HD Computation: $\server$ computes $\enc{HD_{\mathbf{x,y}}}$
    using procedure \ref{sub:ciphertext_packing}. We note that the noise term $\mathbf{e_{HD}}$ of
    $\enc{\mathbf{HD,e_{HD}}}$ can leak information
    about $\mathbf{x}$ when $\mathbf{HD}$ is decrypted. Therefore,
    an extra step is needed to secure the operation.
    \begin{itemize}
    \item Sample $\mathbf{e_{r}} \randomsample \chi_{r}^n$ such that $\norminf{\mathbf{e_r}}$ is big enough compared to
      $\norminf{\mathbf{e_{HD}}}$.(Section \ref{sec:Renyi})
    \item Compute $\enc{\mathbf{r,e_r}}$ and carry out an homomorphic addition operation to mask both the values of
      $\mathbf{HD}$ and the noise $\mathbf{e_{HD}}$:
      $\enc{\mathbf{HD', e'_{HD}}} = \enc{\mathbf{HD, e_{HD}}} + \enc{\mathbf{r,e_r}}$
    \end{itemize}
    The result $\enc{\mathbf{HD'}}$ is then sent to $\user$.
  \item \(\user\) decrypts $\enc{\mathbf{HD'}}$ and derives the actual value ${HD'}$ from the first coefficient of the
    plaintext: \( dec\enc{\mathbf{HD'}} = HD' + r_1 + r_2 + \dots + r_{n-1} \). \(\user\) sends \(HD'\) to \(\server\).
  \item \(\user\) proves that it does the decryption honestly, this is done similarly to step 2.
  \item \(\server\) unmasks \(HD'\) and outputs the authentication result \(HD \stackrel{?}{<} \tau\)
  \end{enumerate}

\end{description}

\begin{figure}[htbp!] 
  \centering \procedure{THE FIRST PROTOCOL}{
    \textbf{Client} \> \> \textbf{Server}\pclb
    \pcintertext[dotted]{Enrolment}\\
    \text{Extract \& Encrypt \textbf{x}} \> \sendmessageright*{\enc{\mathbf{x}}}
    \> \text{Persist $\enc{\mathbf{x}}$}\pclb
    \pcintertext[dotted]{Authentication}\\
    \text{Extract \& Encrypt \textbf{y}} \> \sendmessageright*{\enc{\mathbf{y}}}
    \> \\
  }
  \caption{The First Protocol}
  \label{fig:firstProtocol}
\end{figure}


\subsubsection{Correctness}
\label{sec:correctness}
In the enrolment step, as mentioned before, in order to encrypt $\mathbf{x}$, $\user$ samples
$\mathbf{u,f,g} \randomsample \chi_{\alpha q}^n$ and does $\mathbf{c_0} = \mathbf{p_0u} + t\mathbf{f} + \mathbf{x}$ and
$\mathbf{c_1} = \mathbf{p_1u} + t\mathbf{g}$. The condition for decryption correctness is
$[\langle \mathbf{c_0 + c_1s} \rangle]_q < q/2$, or, $-t\mathbf{eu} + t\mathbf{f} + t\mathbf{gs} < q/2$.

In the authentication step, \(\enc{HD}\) is decryptable if \(\norminf{\langle \enc{HD}, \mathbf{s} \rangle} < q/2\), if
we let \(U\) to be the upper bound of \(\norminf{\langle \enc{HD}, \mathbf{s} \rangle}\), and considering that
\(\norminf{a + b} \leq \norminf{a} + \norminf{b}\) and \(\norminf{a.b} \leq n.\norminf{a}.\norminf{b}\). From theorem
~\ref{theo:HDComputation}, we can derive \(\norminf{\langle \enc{HD},\mathbf{s} \rangle} \leq 2nU + 2nU^{2}\). As in the
work of \cite{naehrig2011can}, we can take \(U\) to be \(2t \sigma^{2} \sqrt{n}\), which is an experimental
estimation. Therefore, the final correctness condition for authentication is \( 16n^{2}t^{2}\sigma^{4} < q\).
\begin{lemma}[Condition for Correct Decryption of HD]
  \label{le:hdcorrectness}
 For the BV encrypted Hamming Distance \(\enc{HD}\), the decryption recovers the correct result if \(\langle \enc{HD},
 \mathbf{s} \rangle\) does not wrap around mod q, namely, if \(16n^{2}t^{2}\sigma^{4} < q\).
\end{lemma}

\subsubsection{Security}
\label{sec:security}
The proposed scheme satisfies the security notions defined in Section \ref{sec:syntaxModel}, proofs are
provided in Appendix \ref{append:Proofs}
\begin{theorem}[Server side security]
  \label{theo:server}
  Under the IND-CPA security of BV cryptosystems, and the zero-knowledge property of the Stern protocol, the proposed
  scheme satisfies (Honest But Curious) Server Privacy Security.
\end{theorem}
\begin{theorem}[Client side security]
  \label{theo:client}
  Under the IND-CPA security of BV cryptosystem and the soundness property of the underlying Stern protocol, the
  proposed scheme satisfies Impersonation Security. Concretely, for $\delta>0$, the protocol is $(q,c)$-secure against
  impersonation with $c \leq c(\delta) + 3 \cdot c_1$, assuming the underlying non-private biometric protocol has
  impersonation probability $\varepsilon_{bio}$ and the underlying Stern ZK protocols have knowledge error
  $\eps_{ZK1},\eps_{ZK2}$ such that
  $q(\varepsilon_{ZK1}+\varepsilon_{ZK2}) + \delta \leq c_1 \cdot
  \varepsilon_{bio}$, $c(\delta) = 2 e^{1+2\delta}$, and the condition $\sigma/r_0 \geq 4 \pi k n q$ holds, with
  $k = 1 + \sqrt{1/\pi \ln(2nq/\delta)}$ and $r_0$ as an upper bound on the size of the noise in $C_{HD}$.
\end{theorem}

\section{Renyi Divergence Analysis technique}
\label{sec:secProcRenyi}
% \subsection{Renyi Divergence and its application in noise masking}
% \label{sec:Renyi_original}

For the following analysis, let $D_1$ be a discrete Gaussian on $\mZ$ with
deviation parameter $\sigma$ shifted by the constant $r_0 \in \mZ$, while $D_2$
is a discrete Gaussian on $\mZ$ with dev. par. $\sigma$ centered on zero,
i.e. $D_1 = D_{\mZ,\sigma} + r_0$ and $D_2 = D_{\mZ,\sigma}$, where
$D_{\mZ,\sigma}(x) = e^{-\pi \cdot x^2/\sigma^2}/\sum_{z \in \mZ} e^{-\pi \cdot
  z^2/\sigma^2}$ for $x \in \mZ$. To allow us to use $RD_{\infty}$ we put
tail-cut variants $D_1^{(cut)}$ and $D_2^{(cut)}$ of $D_1$ and $D_2$ into play,
respectively with parameter $k$. The parameter $k$ defines where $D_1$ and $D_2$
are cut, for example, we can set $k=3$ to cut the distributions at 3
deviation parameters from the mean. So, we let $D_{\mZ,\sigma}^{(cut)}$ denote
distribution $D_{\mZ,\sigma}$ tail-cut to the interval
$[-k \cdot \sigma, k \cdot \sigma]$ by rejection sampling. We let
$D_1^{(cut)} = D_{\mZ,\sigma}^{(cut)} + r_0$ and
$D_2^{(cut)} = D_{\mZ,\sigma}^{(cut)}$. Notice that the supports of
$D_1^{(cut)}$ and $D_2^{(cut)}$ are different, namely
$Supp(D_1^{(cut)}) = [-k\sigma+r_0,k\sigma+r_0]$ while
$Supp(D_2^{(cut)}) = [-k\sigma,k\sigma]$. We assume, without loss of generality,
that $r_0 > 0$. We would like to switch from distribution $D_1^{(cut)}$ to
$D_2^{(cut)}$, but unfortunately $R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)})$
is not finite, since $Supp(D_1^{(cut)})$ is not a subset of
$Supp(D_2^{(cut)})$. To satisfy the latter condition, we first switch from
$D_1^{(cut)}$ to $\overline{D}_1^{(cut)}$ by further cutting (by rejection
sampling) the positive tail of $D_1^{(cut)}$ to ensure it does not go beyond the
$k \sigma$ upper bound on tail of $D_2^{(cut)}$, and use a (mild condition)
statistical distance step to lower bound $\overline{D}_1^{(cut)}(E)$. Then, in a
second step using
$Supp(\overline{D}_1^{(cut)})=[-k\sigma+r_0, k\sigma] \subseteq
[-k\sigma,k\sigma] = Supp(D_2^{(cut)})$, we derive a finite upper bound on
$R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)})$ to lower bound
$D_2^{(cut)}(E)$. Details follow.
\begin{sloppypar}
  \textbf{First SD Step.} Since $Supp(D_1^{(cut)})$ is transformed into
  $\overline{D}_1^{(cut)}$ by rejection and resampling if a sample of
  $Supp(D_1^{(cut)})$ falls in $(k\sigma,k\sigma+r_0]$, we have
  $SD(D_1^{(cut)}, \overline{D}_1^{(cut)}) \leq
  D_1^{(cut)}((k\sigma,k\sigma+r_0]) = D_2^{(cut)}((k\sigma-r_0,k\sigma]) =
  D_{\mZ,\sigma}((k\sigma-r_0,k\sigma])/C_2$, where
  $C_2 = D_{\mZ,\sigma}([-k\sigma,k\sigma])$. Consequently,
$$
\Delta \defeq \frac{D_{\mZ,\sigma}((k\sigma-r_0,k\sigma])}{C_2} = \frac{\sum_{z
    \in (k\sigma-r_0,k\sigma])} \e^{-\pi z^2/\sigma^2}}{\sum_{z \in
    [-k\sigma,k\sigma])} \e^{-\pi z^2/\sigma^2}}.
$$
For the numerator, the upper bound
$\sum_{z \in (k\sigma-r_0,k\sigma])} \e^{-\pi z^2/\sigma^2} \leq
\int^{\infty}_{k\sigma-r_0} \e^{-\pi z^2/\sigma^2} dz \leq \sigma \cdot \e^{-\pi
  (k\sigma-r_0)^2/\sigma^2}$ is obtained by using the standard normal distribution upper bound
$\int^{\infty}_{\gamma} \frac{1}{\sqrt{2\pi} \sigma} \cdot \e^{- z^2/\sigma^2}
dz \leq \e^{-\gamma^2/(2\sigma^2)}$ for $\gamma \geq 0$. For the denominator, the lower bound is
$\sum_{z \in [-k\sigma,k\sigma]} \e^{-\pi z^2/\sigma^2} \geq 2 \cdot \sum_{z \in
  [0,k\sigma])} \e^{-\pi z^2/\sigma^2} \geq 2 \cdot (\int^{\infty}_{0} \e^{-\pi
  z^2/\sigma^2} dz - \int^{\infty}_{k\sigma} \e^{-\pi z^2/\sigma^2} dz) \geq
\sigma \cdot (1 - 2 \cdot \e^{-\pi (k\sigma-r_0)^2/\sigma^2})$. Therefore,
$SD(D_1^{(cut)}, \overline{D}_1^{(cut)}) \leq \Delta \leq \delta' / (1-2\delta')
\leq 2\delta'$ if $\delta' \leq 1/4$, where
$\delta' = \e^{-\pi (k\sigma-r_0)^2/\sigma^2}$. Defining $\delta = 2\delta'$, makes $\Delta \leq \delta$ if $\delta \leq 1/8$ and the conditions
$r_0 \leq \sigma$ and $k \geq 1 + \sqrt{1/\pi \cdot \ln(2/\delta)}$
hold. Therefore, for any event $E$, 
$\overline{D}_1^{(cut)}(E) \geq D_1^{(cut)}(E) - \delta$ is the case.
\end{sloppypar}
% with probability $D_1^{(cut)}(E)$ \geq \varepsilon$, we have can take $\delta
% = \var{\epsilon This tells us how to set the tail cut parameter $k$.

\textbf{Second RD step.} The desired RD of order $\infty$ is defined by
\[
  R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)})) = \max_{x \in
    [-k\sigma+r_0,k\sigma]} \frac{\overline{D}_1^{(cut)}} {D_2^{(cut)}(x)}.
\]
Observe that, for each $x \in [-k\sigma+r_0,k\sigma]$, we have
$\overline{D}_1^{(cut)}(x) = C \cdot D_1^{(cut)}(x)$, where the normalization
constant
$C = \frac{1}{1-D_1^{(cut)}((k\sigma,k\sigma+r_0])} =
\frac{1}{1-D_2^{(cut)}((-k\sigma+r_0,k\sigma])} = \frac{1}{1-\Delta}$, where
$\Delta$ is defined and upper bounded by $\delta$ above, under the assumed
conditions on $k$ and $\delta$. Since the $D_1^{(cut)}(x)$ and $D_2^{(cut)}(x)$
are shifts of each other, they have the same rejection sampling normalization
constant with respect to $D_1$ (resp. $D_2$). Therefore,
$D_1^{(cut)}(x)/D_2^{(cut)}(x)=D_1(x)/D_2(x)$ for each $x$ in the support of
both $D_1^{(cut)}$ and $D_2^{(cut)}$, and thus
% $D_1(x)$ and $D_2(x)$ divided by some constant factor $C_1$ and $C_2$ (to keep
% the cumulative probability area to be 1) . In our context, as $D_1(x)$ and
% $D_2(x)$ are the same distributions with different means, so $C_1 = C_2$.
\begin{align*}
  R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)}) &\leq \frac{1}{1-\delta} \cdot \max_{x \in [-k\sigma+r_0,k\sigma]} \frac{D_1(x)}
                                                  {D_2(x)}\\
                                                &= \max_{x \in [-k\sigma,k\sigma]}\frac{e^{\frac{-\pi(x-r_0)^2}{\sigma^2}}}{e^{-\pi\frac{x^2}{\sigma^2}}}
  \\
                                                &= e^{\pi \cdot r_0^2/\sigma^2} \cdot \max_{x \in [-k\sigma+r_0,k\sigma]}e^{\frac{2\pi r_0}
                                                  {\sigma^2}x}
\end{align*}
This is an exponential function yielding its max value at $x = k\sigma$:
$$
R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)}) = e^{1/(1-\delta)} \cdot e^{\pi
  \cdot r_0^2/\sigma^2 + 2\pi k \cdot r_0/\sigma}.
$$
Since $0<\delta \leq 1/8$, the first factor above is
$\leq 1+2\delta \leq e^{2\delta}$. Also, a simple computation shows that the
second factor is $\leq e$ if the condition $\sigma/r_0 \geq 4 \pi \cdot k$ is
satisfied using $k \geq 1$. We conclude, under the assumed parameter conditions,
that
$R_\infty(\overline{D}_1^{(cut)}\|D_2^{(cut)})) \leq e^{1+2\delta} = c'(\delta)$
is constant for a constant $\delta>0$, so that, by the RD probability preservation
property,
$D_2^{(cut)}(E) \geq \frac{1}{c(\delta)} \cdot \overline{D}_1^{(cut)}(E) \geq
\frac{1}{c(\delta)} \cdot (D_1^{(cut)}(E) - \delta)$. Note that if
$D_1^{(cut)}(E)=\varepsilon$, then, by choosing $\delta = \varepsilon/2$, we get
$D_2^{(cut)}(E) \geq \frac{1}{2c(\delta)} \cdot \varepsilon$, and we only need
$k \geq 1 + \sqrt{1/\pi \cdot \ln(2/\delta)}$ and $\sigma/r_0$ logarithmic in
$1/\delta$, much smaller than $\sigma/r_0$ linear in $1/\delta$, which we would
need if we were to use the `SD only' analysis approach.

The above discussion immediately generalizes from the one-dimensional case of
discrete Gaussian samples over $\mZ$ to the $m$-dimensional case of discrete
Gaussian samples over $\mZ^m$, due to the independence of the $m$
coordinates. The only change to the above argument is that the statistical
distance in the `SD step' can multiply by at most a factor $m$, whereas the RD
in the `RD step' above gets raised to the $m$'th power, where we replace $r_0$
by $\|\vec{r}_0\|_{\infty}$. We compensate it by replacing the bound
$\delta$ on $\Delta$ in the above analysis by the bound $\delta/m$. We have
therefore proved the following result used in our impersonation security proof,
which improves upon the $R_2$-based analogue result for shifted Gaussians stated
in~\cite{langlois2014gghlite}.

% \keq Note that when $r_0$ is much smaller than $\sigma$, the first term
% $e^{\frac{\pi r_0^2}{\sigma^2}} \approx 1$. However, if $r_0$ is too small
% compared to $\sigma$, then the second term is at most $e^{2k\pi}$, which is
% also very large in the relation of equation (\ref{eq:renyi}). Or, we can also
% choose parameters such as $k = 3$ and $\frac{r_0}{\sigma} = \frac{1}{6}$, then
% the trade-off factor becomes much smaller, $e^{\pi}$. In conclusion, by
% applying infinity order RD to measure the truncated distributions closeness,
% we might obtain better parameters ($\sigma \geq 6r_0$, for example, compared
% to $\sigma \geq Kr_02^\lambda$ in SD approach) while providing
% \textit{Probability Preservation}, or security against distinguishing
% adversaries.

\begin{lemma} \label{le:Renyi} For integer $m \geq 1$, real
  $\sigma>0$,$k \geq 1$, real $0<\delta \leq 1/8$ and vector
  $\vec{r}_0 \in \mZ^m$, let
  $D_1^{(cut)} = D_{\mZ^m,\sigma}^{(cut)} + \vec{r}_0$ and
  $D_2^{(cut)} = D_{\mZ^m,\sigma}^{(cut)}$ be relatively shifted tail-cut
  discrete Gaussian distributions, where $D_{\mZ^m,\sigma}^{(cut)}$ is the
  discrete Gaussian $D_{\mZ^m,\sigma}$ with its tails cut to the support
  $[-k\sigma,k\sigma]^m$ by rejection sampling. If the conditions
  $k \geq 1 + \sqrt{1/\pi \cdot \ln(2m/\delta)}$ and
  $\sigma/\|\vec{r}_0\|_{\infty} \geq 4 \pi \cdot k \cdot m$ hold, then, for any
  event $E$ defined over the support of $D_1^{(cut)}$, it is the case that
$$
D_2^{(cut)}(E) \geq \frac{1}{2e^{1+2\delta}} \cdot \left(D_1^{(cut)}(E) - \delta\right).
$$
\end{lemma}

\subsection{Correctness Analysis}
\label{sec:2correctness}

We start with the noise sampled during key generation:
\[e_{0} \randomsample \chi_{\alpha q}\] where \(\chi\) is typically a Gaussian
distribution with standard deviation \(\alpha q\). The noise bound of the first
level ciphertext \(c = (\mathbf{c_{0}, c_{1}})\) would be
\(B_{0} = \norminf{[<\mathbf{c,s}>]_{q}}\). Thus,

\begin{align*}
  \left[ \langle \mathbf{c, s} \rangle \right]_{q} &= \mathbf{p_{0}u} + t \mathbf{g} + \mathbf{m} + \mathbf{p_{1}us} + t \mathbf{fs}\\
                                                   &= m + t(\mathbf{g + fs -e_{0}u})
\end{align*}

So we can approximately bound \(B_{0} \leq t \norminf{e_{0}}^{2}\). The noise of
the HD ciphertext (unmasked) \(\norminf{e_{HD}}\) can be bound by
\(2nB_{0} + n B_{0}^{2}\), as
\[
  enc(HD) = enc_{1}(\mathbf{T})C_{1} + enc_{2}(\mathbf{Q})C_{2} - 2
  enc_{1}(\mathbf{T})enc_{2}(\mathbf{Q})
\]
The final masked ciphertext will have noise
\[
\norminf{e_{HDM}} \leq (4\pi kn + 1)n(B_{0}^{2} + 2B_{0})
\]
Where \(k = 1 + \sqrt{\frac{1}{\pi \log{4n FAR^{-1}}}}\). We can derive the
final correctness condition:
\[
q > 4 \pi n^{2} t (B_{0}^{2} + 2B_{0})
\]

Notation
\begin{itemize}
\item \(\alpha q\) standard deviation of the original noise distribution, assume
  Gaussian Distribution \(\chi\)
\item \(e_{0}\) original noise used in key generation and original encryption
  ($(\mathbf{p_0},\mathbf{p_1})$ where $\mathbf{p_1} \randomsample R_Q$ and
  $\mathbf{p_0} = -(\mathbf{p_1}\mathbf{s} + t\mathbf{e})$ with
  $\mathbf{e} \randomsample \chi$)
\item \(B_{0}\) noise of the first level ciphertext
  \[
    Enc_{pk}(\mathbf{m}) = (\mathbf{c_0},\mathbf{c_1}) = (\mathbf{p_0}\mathbf{u}
    + t\mathbf{g} + \mathbf{m}, \mathbf{p_1}\mathbf{u} + t\mathbf{f})
  \]
\end{itemize}
  
 


\section{Results}
\label{sec:secProcResult}
- In the previous chapter, the following parameters were used for the
cryptosystem. blah blah: n, q, $\alpha q$, etc. This configuration does not
cover circuit privacy as the error noise leaks information about the registered
template. If the leakage is to be covered by the added noise with magnitude
computed from Statistical Distance approach, we would need the noise added to be

(blah, compute the noise)

With that new noise added, the parameter $q$ of the system need to be adjusted
to ensure correctness. According to blah , $q$ needs to be

(blah, compute new q)

With this new moduli, the overheads introduced to both computation and
communication are significant. For example, in the ZKP protocol, the
communication size of each round would be (blah) and could not be considered
practical with the current infrastructure bandwidth.

However, by applying the approach in this chapter, the new noise only needs to
be (blah), with the proposed parameter, the new moduli is approximately blah
bits and the communication overhead increase from 8MB to 16MB for the whole
protocol. This is a lot smaller than SD approach and can be considered practical
in some of the current deployed networks (such as 100 Mbps cable internet or
NBN).

In conclusion, by using Renyi Divergence as an alternative to measure the
closeness of distributions in stead of Statistical Distance in security proofs
of lattice-based cryptography, one can efficiently reduce the parameters'
sizes. This chapter introduced how a specific order of RD (infinity order) can
effect positively to the parameters' choices for the cryptosystem. In the next
chapter, we start improving the security model by hiding the value of the
distance from the server, all the results of this chapter will be kept during
the circuit privacy covering steps as neccessary in any steps of the server side.


    
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
