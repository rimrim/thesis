%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}  %Title of the First Chapter

\ifpdf
    \graphicspath{{Chapter1/Figs/Raster/}{Chapter1/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi

\section{User Authentication and Biometrics}
\label{sec:biometricIntro}
User authentication is the process of verifying the claimed identity of a user,
which is an important aspect in the big picture of information and network
security. Generally, there are three factors that can be used for this purpose:
Something You Know (SYK), such as password; Something You Have (SYH), such as
smartcard and Something You Are (SYA), such as iris or fingerprint. The last
factor, also known as biometrics, can be considered the most usable factor as
one does not have to carry or remember anything during authentication.
Biometric authentication is lately deployed widely in many platforms thanks to
the reduced cost of hardware.

Various traits from body have been proposed and used for person recognition and
verification. The three most popular traits deployed are fingerprint, face and
iris modalities. Traits such as palmprint, hand geometry, voice are deployed in
some commercial applications. There are many other traits proposed by
researchers (such as gait, ear, keystroke dynamics, etc) but yet to attain
sufficient level of technological maturity for deployment. Regardless of the
traits used, in biometrics verification systems, there are generally 2 stages:
enrollment and authentication. In the enrollment stage, a user registers his
biometric template with a server, later in the authentication stage, he extracts
his template again and submit to the server to authenticate. Unlike password
authentication where a user always enters the same password he used when
registered, in biometrics system, the query template of a genuine user is only
very similar to the registered one due to many factors (for instance, the user
might touch the fingerprint sensor in different corners when
authenticating). The server has to compute some kind of distance between the
registered and the query templates and compares the result with some threshold
values to decide the authentication results. Compared to SYK and SYH, the SYA
factor has these 2 parameters, namely False Acceptance Rate (FAR) and False
Rejection Rate(FRR): FAR is the rate that the system wrongly accepts an
impersonating user, FRR is the rate that the system rejects a genuine
user. Different biometrics authentication systems use different extraction
methods and distance measures to reduce FAR and FRR, this is also an active
research area. In this thesis, we do not focus on template extraction techniques
or improving FAR and FRR but we are interested on methods to provide extra
privacy and security features to any methods that are based on Hamming Distance
of biometrics represented by bitstrings.

\section{Privacy-preserving Biometric Authentication}
\label{sec:privacyLiteratures}
Many biometrics authentication systems do not have protection for the data
stored in server. A server breach incident would result a catastrophic
consequence (in 2015, nearly 6 millions of plaintext fingerprint data of US
goverment employees were leaked due to a security attack
\cite{OPMsays563:online}). We see that the main drawback of SYA authentication
is lack of biometric privacy against server exposure. If biometric data is ever
revealed or stolen, the victims may be vulnerable to impersonation attacks for
the rest of their life, as it is nearly impossible to change oneâ€™s fingerprints
or iris, unlike password or smartcard approaches. Hence there is a strong
motivation to develop authentication systems that protect the privacy of
biometric data. There are four main approaches to privacy preserving biometrics
authentication at present (details avalable in surveys of \cite{jain201650} and
\cite{jain2008biometric}) \begin{itemize} \item Transforming biometric data
  using a key-less many-to-one transformation, also known as `Non-Invertible'
  transforms (e.g.~\cite{ratha2007generating}): Since the function is key-less,
  biometric impersonation security under server exposure is vulnerable to
  off-line brute-force attacks, also known as FAR
  attacks~\cite{uludag2004attacks,roberts2007biometric}, that are the biometric
  analogue of off-line dictionary attacks on weak password hashing. Moreover,
  the many-to-one mapping trades-off the biometric accuracy (FRR) performance of
  the biometric scheme. A solution to overcome such issue is to introduce a
  cryptographic secret key authentication factor, i.e transforming the
  registered biometric template stored on the server by encrypting it with a
  secret key stored at the client's side. With this second authentication
  factor, verification will require the secret key, which prevents brute-force
  attacks. Generally speaking, two-factor security can be obtained if the system
  is secure when either the secret key is exposed (e.g., client's device is
  stolen), or if the encrypted biometric stored in the server is leaked (e.g.,
  server breach). All other approaches we consider below are based on this
  two-factor method.


\item Transforming biometric data using a keyed `distance-preserving' encryption scheme:  Here, the distance between the queried and registered biometric templates is compared by the server using the encrypted query and registered templates.  Typical techniques in this category include
            cancelable biometrics and biohashing (\cite{teoh2008cancellable}, \cite{jin2004biohashing}, 2P-MCC (\cite{cappelli2010minutia}). %The security of this approach is unclear that was not based on standard cryptographic assumptions.
). However, the security of such encryption schemes is unclear as they are based on heuristic and non-standard cryptographic assumptions, and several have been found to be insecure (e.g.~\cite{lee2009inverse,lacharme2013preimage}).

%\item
%            depends on the security of the key, this is the main
%            limitations of the approach:
%        the user's privacy is not assured
%    as the server decrypts and read the biometrics data when doing
%authentication, or a malicious server can even brute-force the key and/or the biometrics itself (Hill-climbing attack).
\item Biometric cryptosystems / Fuzzy hashing: This approach is based on error
    correcting codes techniques (\cite{uludag2004biometric}, \cite{nagar2010hybrid}) to extract a noiseless cryptographic key from a noisy biometric. However, depending on parameter settings, these techniques may leak significant information, and the tradeoff between biometric accuracy (False Acceptance Rate and False Rejection Rate) and security are
    controversial or not well understood, and in practice the technique may  needs strong restriction of accuracy. In particular, the underlying error
correcting codes can only handle a specific range of the threshold depending on the usability parameters.

\item Secure Computation / Homomorphic encryption techniques
    (\cite{yasuda2014practical}, \cite{shahandashti2012private},
    \cite{higo2015privacy}): In this approach, the biometrics data is encrypted
    in the server using a homomorphic encryption scheme. The operations on
    encrypted data would measure the similarity between the request and the
    stored templates. This approach has the potential to overcome the heuristic
    security issues of the above approaches (by using an encryption scheme based
    on standard cryptographic assumptions), as well as the biometric accuracy
    issues (by implementing homomorphically the same verification check as the
    underlying biometric scheme). However, existing protocols following this
    approach suffer from other significant drawbacks. In particular, earlier
    approaches~\cite{shahandashti2012private} were based on the Paillier
    \emph{additive} homomorphic scheme and offer a limited efficiency due to
    both the inefficiency of homomorphic operations of the Pailler scheme, and
    the extra protocol overhead required to handle the `addition-only'
    homomorphism limitation of the Paillier scheme. Besides, Paillier system
    does not provide \emph{long term} security (it is not quantum resistent).
    The efficiency is
    significantly improved by a recent protocol of Yasuda et
    al.~\cite{yasuda2014practical} based on lattice-based SomeWhat Homomorphic
    encryption (SWHE) supports both homomorphic additions and multiplications.
    The cryptosystem used is believed to be quantum-resistant as
    well.
    However, this protocol requires the use of a trusted-third party server to
    verify the final authentication result: The main server stores encrypted
    biometric and compute HD homomorphically while the other server stores the
    decryption key and decrypts/verifies the HD.
    As a result, it is not secure under
    server exposure attack (as keys from trusted server can decrypt data on the
    main one).
  \end{itemize}

  We focus our work on the fourth approach with different improvements, the goal
  is to design authentication protocol that satisfies more complex security and
  usability requirements according to the current trends. The requirements include:
\begin{description}
\item [Privacy against server exposure.] The biometric data should be securely
  encrypted prior to uploading to the server, using a key stored only to the
  biometric owner. Ideally, the server storing the encrypted biometric should
  not be able to comprehend the data, in other words, it does not have the
  encryption key.

\item [Quantum resistant privacy. ] Considering quantum computing is developing
  fast and the biometric data will persist over the lifetime of a user, the
  encrypted biometric data should have long term security against quantum
  computing attacks.
\item [No reliance on trusted third parties. ] With the wide exposure of cloud
  computing to potential attacks, the protocol should not rely on one or more
  cloud-based trusted third parties to help with the authentication process.
\item [Security against malicious client.] An attacker attempting to impersonate
  the real client to the server should not be able to authenticate without a
  genuine biometric, even if the attacker is malicious and does not follow the
  authentication protocol, i.e. the impersonation attacker cannot be assumed to
  be `honest but curious'(HBC).
\item [Two factor security.] If the client is responsible for decrypting
  biometric-related data, the protocol should be multi-factor secure: an
  attacker with a compromised key should not be able to authenticate without a
  genuine biometric.
\item [Practical performance.] The computation time and communication size of
  the whole protocol should be within practical time frame.
\end{description}
Previous protocols in the literature do not meet one or more of our
requirements. In particular, many previous protocols
(\cite{bringer2007application}, \cite{erkin2009privacy},
\cite{osadchy2010scifi}) assume honest-but-curious clients and are insecure in
the authentication context that involves malicious clients. The few protocols
involving malicious clients (\cite{shahandashti2012private},
\cite{vsedvenka2015secure}) are not quantum-resistant. Previous practical
quantum-resistant protocols (\cite{yasuda2014practical},
\cite{mandal2015comprehensive}) are not secure against malicious client and
involve the use of a trusted third-party verification server and therefore
client privacy is not completely achieved.

\section{Contributions of the thesis}
\label{sec:thesisContributions}
We propose different protocols to support above requirements. Our
technique combines state-of-the-art cryptographic tools such as Homomorphic
Encryption (HE) and Zero-Knowledge-Proof (ZKP) to balance security and usability
of the system. The techniques are all lattice-based, which is currently the best
candidate for long term security against quantum attacks. Our protocol also
achieves privacy against server exposure without relying on a trusted third
party. The contributions include:
\begin{itemize}
\item Quantum resistant and provable-secure biometric authentication protocols
  that does not rely on trusted third-parties. (Section
  \ref{sub:the_new_protocol}). The server stores the encrypted data and does
  homomorphic operations to compute the distance (HD) to decide the
  authentication result. It does not need a third party to decrypt the encrypted
  HD but sending it to the client for decryption. The client uses ZKP to
  convince the server that the ciphertext was decrypted correctly.
\item The protocols provide security under malicious client model, this is done
  by new ZKP techniques that we design specifically for different ciphertext
  packing methods . It is also applicable to do proof of plaintext knowledge for
  the BGV Homomorphic Encryption scheme (\cite{brakerski2011fully}) that we
  adapted. We have different ZKP variants are based on \cite{stern1993new} and
  \cite{schnorr1989efficient}, which are the two main approaches for Zero
  Knowledge Proof.
\item Due to the noise inherent inside lattice-based homomorphic encryption and
  its correlation with the evaluated ciphertext, we observe that there can be
  information leakage about the original plaintexts used in the homomorphic
  computations to a two-factor attacker that exposed the client's secret key. We
  propose an approach to cover such leakage without significantly reducing the
  efficiency of the protocol: the approach is a new application of Renyi
  Divergence (RD) based analysis to show the security of the protocol with a
  small 'imperfect' one-time pad. (Section \ref{sec:Renyi}). This correlation of
  Homomorphic Encryption noise with the original plaintext before homomorphic
  evaluation was observed as a problem of "circuit privacy" in theoretical HE
  literature (\cite{sander1999non}, \cite{ishai2007evaluating}), but the
  proposed solutions (\cite{homenc}, \cite{ostrovsky2014maliciously},
  \cite{gentry2010hop}) involves 'smudging' (imperfect masking) or bootstrapping
  techniques with an exponentially large noise (in the security parameter) that
  reduces efficiency. In contrast, our Renyi-based method can use much smaller
  imperfect masks leading to better efficiency. This is the first application of
  Renyi divergence techniques to circuit privacy of HE to our knowledge.
\item We propose a new Oblivious Transfer (OT) approach to obtain inputs to
  Garbled Circuits. Unlike traditional OT approaches where the receiver obtains
  keys as inputs to a garbled circuit, our approach is compatible with the
  undernearth homomorphic cryptosystem and obtain nicely the encryptions of the
  keys to the circuit. The approach is very effective with only one level of
  homomorphic multiplication operation and can be applied to other application
  scenarios where secure multy-parties computation is required.
\end{itemize}

\section{Organization of the thesis}
\label{sec:introOrganize}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
