%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}  %Title of the First Chapter

\ifpdf
    \graphicspath{{Chapter1/Figs/Raster/}{Chapter1/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi

\section{User Authentication and Biometrics}
\label{sec:biometricIntro}
User authentication is the process of verifying the claimed identity of a user,
which is an important aspect in the big picture of information and network
security. Generally, there are three factors that can be used for this purpose:
Something You Know (SYK), such as password; Something You Have (SYH), such as
smartcard and Something You Are (SYA), such as iris or fingerprint. The last
factor, also known as biometrics, can be considered the most usable factor as
one does not have to carry or remember anything during authentication.
Biometric authentication is lately deployed widely in many platforms thanks to
the reduced cost of hardware.

Various body features have been proposed and used for recognition and
verification of a person's identity, the three most popular ones being fingerprints, face and
iris modalities. Some commercial applications enforce other recognition techniques, based on palm-prints, hand geometry and voice. There are many other idiosyncracies proposed by
researchers (such as gait, ear, keystroke dynamics, etc.) but yet to attain a
sufficient level of technological maturity for deployment. Regardless of the
traits used, in biometrics verification systems, there are generally two stages:
enrollment and authentication. In the enrollment stage, a user registers his
biometric template on a server. In the authentication stage, he extracts
his template again and submits it to the server to prove his identity. Unlike password
authentication, where a user always enters the same password he chose when
registering, in biometric systems, the query template of a genuine user is not exactly the same, but only very similar to the registered one, as, for instance, the user
might touch the fingerprint sensor on different corners when
authenticating. The server has to compute some kind of distance between the
registered and the query templates and compares the result with some threshold
values to decide on the authentication results. Compared to SYK and SYH, the SYA
factor has two parameters, namely, False Acceptance Rate (FAR) and False
Rejection Rate (FRR): FAR is the rate the rate at which the system wrongly accepts an
impersonating user, FRR is the rate the rate at which rejects a genuine
user. Different biometrics authentication systems use different extraction
methods and distance measures to reduce FAR and FRR (this is also an active
research area). In this thesis, we do not focus on template extraction techniques
or on improving FAR and FRR; instead, we are interested on means providing extra
privacy and security features to any methods based on Hamming Distance
of biometrics represented by bitstrings.

\section{Privacy-preserving Biometric Authentication}
\label{sec:privacyLiteratures}
Many biometrics authentication systems lack of protection for the data
stored on a server. A server breach incident can thus result in catastrophic
consequences: in 2015, nearly 6 millions plaintext fingerprint data tokens of US
government employees were leaked due to a security attack
\cite{OPMsays563:online}. When biometric data is revealed or stolen, the victims become vulnerable to impersonation attacks for the rest of their lives, as, unlike password or smartcards, oneâ€™s fingerprints or iris are nearly impossible to change. Biometric privacy poorly protected against server exposure can therefore be considered as the main drawback of SYA authentication. SYA proving nonetheless the most convenient authentication method from the usability point of view, a strong
motivation arose to enhance it with an adequate privacy protection of
biometric data. There are four main approaches to privacy-preserving biometrics
authentication at present (details avalable in surveys of \cite{jain201650} and
\cite{jain2008biometric}) \begin{itemize} \item Transforming biometric data
  using a key-less many-to-one transformation, also known as `Non-Invertible'
  transforms (e.g.~\cite{ratha2007generating}): Since the function is key-less,
  biometric impersonation security under server exposure is vulnerable to
  off-line brute-force attacks, also known as FAR
  attacks~\cite{uludag2004attacks,roberts2007biometric}, which are the biometric
  analogue of off-line dictionary attacks on weak password hashing. Moreover,
  the many-to-one mapping trades off the biometric accuracy (FRR) performance of
  the biometric scheme. A solution to overcome such an issue is to introduce a
  cryptographic secret-key authentication factor, i.e., to transform the
  registered biometric template stored on the server by encrypting it with a
  secret key stored at the client's side. With this second authentication
  factor, verification will require the secret key, which prevents brute-force
  attacks. Generally speaking, two-factor security can still protect the system when either the secret key is exposed (e.g., the client's device is
  stolen), or the encrypted biometric data stored in the server is leaked (e.g.,
  server breach). All other approaches we consider below are based on this
  two-factor method.


\item Transforming biometric data using a keyed `distance-preserving' encryption scheme:  Here, the distance between the biometric query template and the biometric registered one is measured by the server using encrypted versions of them.  Typical techniques in this category include
            cancelable biometrics and biohashing (\cite{teoh2008cancellable}, \cite{jin2004biohashing}, 2P-MCC (\cite{cappelli2010minutia}). %The security of this approach is unclear that was not based on standard cryptographic assumptions.
However, the security of such encryption schemes is unclear as they are based on heuristic and non-standard cryptographic assumptions, several of them having been found to be insecure (e.g.~\cite{lee2009inverse,lacharme2013preimage}).

%\item
%            depends on the security of the key, this is the main
%            limitations of the approach:
%        the user's privacy is not assured
%    as the server decrypts and read the biometrics data when doing
%authentication, or a malicious server can even brute-force the key and/or the biometrics itself (Hill-climbing attack).
\item Biometric cryptosystems / Fuzzy hashing: This approach is based on error
    correcting codes techniques (\cite{uludag2004biometric}, \cite{nagar2010hybrid}) to extract a noiseless cryptographic key from a noisy biometric. However, depending on parameter settings, these techniques may leak significant information, and the tradeoff between biometric accuracy (False Acceptance Rate and False Rejection Rate) and security are
    controversial or not well understood, and in practice the technique may  needs strong restriction of accuracy. In particular, the underlying error
correcting codes can only handle a specific range of the threshold depending on the usability parameters.

\item Secure Computation / Homomorphic encryption techniques
    (\cite{yasuda2014practical}, \cite{shahandashti2012private},
    \cite{higo2015privacy}): In this approach, the biometrics data is encrypted
    in the server using a homomorphic encryption scheme. The operations on
    encrypted data would measure the similarity between the request and the
    stored templates. This approach has the potential to overcome the heuristic
    security issues of the above approaches (by using an encryption scheme based
    on standard cryptographic assumptions), as well as the biometric accuracy
    issues (by implementing homomorphically the same verification check as the
    underlying biometric scheme). However, existing protocols following this
    approach suffer from other significant drawbacks. In particular, earlier
    approaches~\cite{shahandashti2012private} were based on the Paillier
    \emph{additive} homomorphic scheme and offer a limited efficiency due to
    both the inefficiency of homomorphic operations of the Pailler scheme, and
    the extra protocol overhead required to handle the `addition-only'
    homomorphism limitation of the Paillier scheme. Besides, Paillier system
    does not provide \emph{long term} security (it is not quantum resistent).
    The efficiency is
    significantly improved by a recent protocol of Yasuda et
    al.~\cite{yasuda2014practical} based on lattice-based SomeWhat Homomorphic
    encryption (SWHE) supports both homomorphic additions and multiplications.
    The cryptosystem used is believed to be quantum-resistant as
    well.
    However, this protocol requires the use of a trusted-third party server to
    verify the final authentication result: The main server stores encrypted
    biometric and compute HD homomorphically while the other server stores the
    decryption key and decrypts/verifies the HD.
    As a result, it is not secure under
    server exposure attack (as keys from trusted server can decrypt data on the
    main one).
  \end{itemize}

  We focus our work on the fourth approach with different improvements, the goal
  is to design authentication protocol that satisfies more complex security and
  usability requirements according to the current trends. The requirements include:
\begin{description}
\item [Privacy against server exposure.] The biometric data should be securely
  encrypted prior to uploading to the server, using a key stored only to the
  biometric owner. Ideally, the server storing the encrypted biometric should
  not be able to comprehend the data, in other words, it does not have the
  encryption key.

\item [Quantum resistant privacy. ] Considering quantum computing is developing
  fast and the biometric data will persist over the lifetime of a user, the
  encrypted biometric data should have long term security against quantum
  computing attacks.
\item [No reliance on trusted third parties. ] With the wide exposure of cloud
  computing to potential attacks, the protocol should not rely on one or more
  cloud-based trusted third parties to help with the authentication process.
\item [Security against malicious client.] An attacker attempting to impersonate
  the real client to the server should not be able to authenticate without a
  genuine biometric, even if the attacker is malicious and does not follow the
  authentication protocol, i.e. the impersonation attacker cannot be assumed to
  be `honest but curious'(HBC).
\item [Two factor security.] If the client is responsible for decrypting
  biometric-related data, the protocol should be multi-factor secure: an
  attacker with a compromised key should not be able to authenticate without a
  genuine biometric.
\item [Practical performance.] The computation time and communication size of
  the whole protocol should be within practical time frame.
\end{description}
Previous protocols in the literature do not meet one or more of our
requirements. In particular, many previous protocols
(\cite{bringer2007application}, \cite{erkin2009privacy},
\cite{osadchy2010scifi}) assume honest-but-curious clients and are insecure in
the authentication context that involves malicious clients. The few protocols
involving malicious clients (\cite{shahandashti2012private},
\cite{vsedvenka2015secure}) are not quantum-resistant. Previous practical
quantum-resistant protocols (\cite{yasuda2014practical},
\cite{mandal2015comprehensive}) are not secure against malicious client and
involve the use of a trusted third-party verification server and therefore
client privacy is not completely achieved.

\section{Contributions of the thesis}
\label{sec:thesisContributions}
We propose different protocols to support above requirements. Our
technique combines state-of-the-art cryptographic tools such as Homomorphic
Encryption (HE) and Zero-Knowledge-Proof (ZKP) to balance security and usability
of the system. The techniques are all lattice-based, which is currently the best
candidate for long term security against quantum attacks. Our protocol also
achieves privacy against server exposure without relying on a trusted third
party. The contributions include:
\begin{itemize}
\item Quantum resistant and provable-secure biometric authentication protocols
  that does not rely on trusted third-parties. (Section
  \ref{sub:the_new_protocol}). The server stores the encrypted data and does
  homomorphic operations to compute the distance (HD) to decide the
  authentication result. It does not need a third party to decrypt the encrypted
  HD but sending it to the client for decryption. The client uses ZKP to
  convince the server that the ciphertext was decrypted correctly.
\item The protocols provide security under malicious client model, this is done
  by new ZKP techniques that we design specifically for different ciphertext
  packing methods . It is also applicable to do proof of plaintext knowledge for
  the BGV Homomorphic Encryption scheme (\cite{brakerski2011fully}) that we
  adapted. We have different ZKP variants are based on \cite{stern1993new} and
  \cite{schnorr1989efficient}, which are the two main approaches for Zero
  Knowledge Proof.
\item Due to the noise inherent inside lattice-based homomorphic encryption and
  its correlation with the evaluated ciphertext, we observe that there can be
  information leakage about the original plaintexts used in the homomorphic
  computations to a two-factor attacker that exposed the client's secret key. We
  propose an approach to cover such leakage without significantly reducing the
  efficiency of the protocol: the approach is a new application of Renyi
  Divergence (RD) based analysis to show the security of the protocol with a
  small 'imperfect' one-time pad. (Section \ref{sec:Renyi}). This correlation of
  Homomorphic Encryption noise with the original plaintext before homomorphic
  evaluation was observed as a problem of "circuit privacy" in theoretical HE
  literature (\cite{sander1999non}, \cite{ishai2007evaluating}), but the
  proposed solutions (\cite{homenc}, \cite{ostrovsky2014maliciously},
  \cite{gentry2010hop}) involves 'smudging' (imperfect masking) or bootstrapping
  techniques with an exponentially large noise (in the security parameter) that
  reduces efficiency. In contrast, our Renyi-based method can use much smaller
  imperfect masks leading to better efficiency. This is the first application of
  Renyi divergence techniques to circuit privacy of HE to our knowledge.
\item We propose a new Oblivious Transfer (OT) approach to obtain inputs to
  Garbled Circuits. Unlike traditional OT approaches where the receiver obtains
  keys as inputs to a garbled circuit, our approach is compatible with the
  undernearth homomorphic cryptosystem and obtain nicely the encryptions of the
  keys to the circuit. The approach is very effective with only one level of
  homomorphic multiplication operation and can be applied to other application
  scenarios where secure multy-parties computation is required.
\end{itemize}

\section{Organization of the thesis}
The thesis is organized as follows:
\begin{description}
\item In Chapter \ref{chap:definitions}, we describe the general notations and
  recall the basics of Lattice-based cryptography, Authentication, Zero
  Knowledge Proof and Secure Multiparties Protocols that are used throughout the
  thesis.
\item In Chapter \ref{chap:firstProtocol}, we present our first variant of the
  protocol that removes the role of trusted third parties in a privacy
  preserving authentication system.
\item In Chapter \ref{chap:renyiDivergence}, we present the new technique to
  cover circuit privacy while still keeping the parameters of the homomorphic
  cryptosystems being in practical thresholds.
\item In Chapter \ref{chap:thirdProtocol}, we construct the third variant of the
  protocol to do both of the tasks computing and comparing the Hamming Distance
  homomorphically (The previous protocols only compute HD on ciphertexts).
\item In Chapter \ref{chap:fourthProtocol}, we present the last variant of the
  protocol, which removes the computation and communication overheads with the
  extra cost during the initialization stage (one time only cost).
\item In the last Chapter, we conclude the thesis and stat the open problems for
  future research.
\end{description}
\label{sec:introOrganize}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
